{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT from HuggingFace Transformers on the IMDB Dataset\n",
    "> __Note__: In this tutorial we use a smaller version of BERT, called __DistilBert__, that is easier and faster to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "from podium.datasets import IMDB, Iterator\n",
    "from podium.models import Experiment\n",
    "from podium.models.impl.pytorch import TorchTrainer, TorchModel\n",
    "from podium.pipeline import Pipeline\n",
    "from podium.storage import Field, LabelField, Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the fields\n",
    "\n",
    "* text - applies `BertTokenizer` to the instance data. We don't store `attention_mask` to reduce the memory footprint of the dataset, instead we create it ourselves on the fly.\n",
    "* label - stores binary labels that represent the sentiment of an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fields():\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    def text_to_tokens(string):\n",
    "        input_ids = tokenizer(string,\n",
    "                              max_length=128,\n",
    "                              padding=False,\n",
    "                              truncation=True,\n",
    "                              return_attention_mask=False\n",
    "                             )['input_ids']\n",
    "        \n",
    "        return tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "    text = Field(name='text',\n",
    "                 tokenizer=text_to_tokens,\n",
    "                 custom_numericalize=tokenizer.convert_tokens_to_ids,\n",
    "                 padding_token=0)\n",
    "    \n",
    "    label = LabelField(name='label', vocab=Vocab(specials=()))\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'label': label\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will be using `DistilBertForSequenceClassification`. This model has additional layers on top of the base model, `DistilBertModel`, to perform classification and this layers are randomly initialized. So we define a function that will return a copy of the same instance of the model each time it gets called. We need this later to perform the comparison between the original, pretrained model and the model that is fine-tuned on a down-stream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def bert_initializer():\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', \n",
    "                                                                return_dict=True)\n",
    "    def get_bert_model():\n",
    "        return copy.deepcopy(model)\n",
    "    \n",
    "    return get_bert_model\n",
    "\n",
    "get_bert_model = bert_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DistilBertForSequenceClassification` is a standard PyTorch `Module` so it can be easily wrapped in another `Module` that has a proper interface to Podium - the model has to return a dictionary that has a key `pred` that points to the model predictions. As mentioned earlier, we are creating the attention mask ourselves so this is a good place to do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelWrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = get_bert_model()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_mask = (x != 0).long()\n",
    "        return_dict = self.model(x, attention_mask)\n",
    "        return_dict['pred'] = return_dict['logits']\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84.1M/84.1M [00:08<00:00, 9.56MB/s]\n"
     ]
    }
   ],
   "source": [
    "fields = create_fields()\n",
    "imdb_train, imdb_test = IMDB.get_dataset_splits(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the loaded data \n",
    "\n",
    "> __Note__: `None` values in the output are for caching purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, ['[CLS]', 'dominic', '##k', '(', 'nicky', ')', 'luciano', 'wears', 'a', \"'\", 'hulk', \"'\", 't', '-', 'shirt', 'and', 'tr', '##udge', '##s', 'off', 'everyday', 'to', 'perform', 'his', 'duties', 'as', 'a', 'garbage', 'man', '.', 'he', 'uses', 'his', 'physical', 'power', 'in', 'picking', 'up', 'other', \"'\", 's', 'trash', 'and', 'hauling', 'it', 'to', 'the', 'town', 'dump', '.', 'he', 'reads', 'comic', '-', 'book', 'hero', 'stories', 'and', 'loves', 'wrestlers', 'and', 'wrestling', ',', 'going', 'to', 'wrestlemania', 'with', 'his', 'twin', 'brother', 'eugene', 'on', 'their', 'birthday', 'is', 'a', 'yearly', 'tradition', '.', 'he', 'talks', 'kindly', 'with', 'the', 'many', 'people', 'he', 'comes', 'in', 'contact', 'with', 'during', 'his', 'day', '.', 'he', 'reads', 'comic', 'books', ',', 'which', 'he', 'finds', 'in', 'the', 'trash', ',', 'with', 'a', 'young', 'boy', 'who', 'he', 'often', 'passes', 'by', 'while', 'on', 'the', 'garbage', 'route', '.', 'unfortunately', ',', 'dominic', '##k', 'has', '[SEP]'])\n",
      "('positive', None)\n"
     ]
    }
   ],
   "source": [
    "print(imdb_train[0].text)\n",
    "print(imdb_train[0].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Podium Experiment\n",
    "\n",
    "To fine-tune the model, we define an `Experiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'lr': 1e-5,\n",
    "    'clip': float('inf'), # disable gradient clipping\n",
    "    'num_epochs': 5,\n",
    "}\n",
    "model_config['num_classes'] = len(fields['label'].vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch]: 781 in 0.10555 seconds, loss=0.43808\n",
      "Total time for train epoch: 205.91309213638306\n",
      "[Valid]: 781 in 0.00576 seconds, loss=0.38564\n",
      "Total time for valid epoch: 63.414066314697266\n",
      "[Batch]: 781 in 0.10557 seconds, loss=0.39357\n",
      "Total time for train epoch: 199.31143307685852\n",
      "[Valid]: 781 in 0.00574 seconds, loss=0.03775\n",
      "Total time for valid epoch: 60.488966941833496\n",
      "[Batch]: 781 in 0.08967 seconds, loss=0.17306\n",
      "Total time for train epoch: 196.89669013023376\n",
      "[Valid]: 781 in 0.00577 seconds, loss=0.15879\n",
      "Total time for valid epoch: 60.47404861450195\n",
      "[Batch]: 781 in 0.10608 seconds, loss=0.04704\n",
      "Total time for train epoch: 199.86573767662048\n",
      "[Valid]: 781 in 0.00578 seconds, loss=0.00348\n",
      "Total time for valid epoch: 60.53778100013733\n",
      "[Batch]: 781 in 0.10526 seconds, loss=0.00671\n",
      "Total time for train epoch: 200.03476238250732\n",
      "[Valid]: 781 in 0.00653 seconds, loss=0.74549\n",
      "Total time for valid epoch: 60.55412006378174\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "iterator = Iterator(batch_size=32)\n",
    "trainer = TorchTrainer(model_config['num_epochs'], device, iterator, imdb_test)\n",
    "\n",
    "# here we have to swap axes to nullify the effect of swapping axes afterwards\n",
    "# because we work with the batch-first model (we should add this option to Podium!!!)\n",
    "feature_transformer = lambda feature_batch: feature_batch[0].astype(np.int64).swapaxes(0, 1)\n",
    "label_transformer = lambda label_batch: label_batch[0].astype(np.int64)\n",
    "\n",
    "experiment = Experiment(TorchModel, \n",
    "                        trainer=trainer, \n",
    "                        feature_transformer=feature_transformer,\n",
    "                        label_transform_fn=label_transformer)\n",
    "\n",
    "experiment.fit(imdb_train,  \n",
    "               model_kwargs={\n",
    "                   'model_class': BertModelWrapper,  \n",
    "                   'criterion': nn.CrossEntropyLoss(),  \n",
    "                   'optimizer': optim.AdamW,\n",
    "                   'device': device,\n",
    "                   **model_config\n",
    "               })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading the fitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for saving/loading the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def load_model(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = experiment.model\n",
    "\n",
    "model_file = 'bert_model.pt'\n",
    "save_model(fitted_model, model_file)\n",
    "loaded_model = load_model(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and making predictions on raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for making predictions with the raw model on the parsed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_to_torch_transformer = lambda t: torch.from_numpy(t[0].astype(np.int64)).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def make_predictions(raw_model, dataset, batch_size=32):\n",
    "    raw_model.eval()\n",
    "    \n",
    "    def predict(batch):\n",
    "        predictions = raw_model(cast_to_torch_transformer(batch))['pred']\n",
    "        return predictions.cpu().numpy()\n",
    "\n",
    "    iterator = Iterator(batch_size=batch_size, \n",
    "                        shuffle=False)\n",
    "    \n",
    "    predictions = []\n",
    "    for x_batch, _ in iterator(dataset):\n",
    "        batch_prediction = predict(x_batch)\n",
    "        predictions.append(batch_prediction)\n",
    "        \n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model comparioson: pretrained BERT vs pretrained + fine-tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_true = imdb_test.batch()\n",
    "y_true = y_true[0].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the pretrained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.5\n",
      "precision score: 0.0\n",
      "recall score: 0.0\n",
      "f1 score: 0.0\n"
     ]
    }
   ],
   "source": [
    "predictions = make_predictions(BertModelWrapper().to(device), imdb_test)\n",
    "y_pred = predictions.argmax(axis=1)\n",
    "\n",
    "print('accuracy score:', accuracy_score(y_true, y_pred))\n",
    "print('precision score:', precision_score(y_true, y_pred, zero_division=0))\n",
    "print('recall score:', recall_score(y_true, y_pred, zero_division=0))\n",
    "print('f1 score:', f1_score(y_true, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the pretrained + fine-tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.87036\n",
      "precision score: 0.8693258875149581\n",
      "recall score: 0.87176\n",
      "f1 score: 0.8705412422608348\n"
     ]
    }
   ],
   "source": [
    "loaded_model_raw = loaded_model.model\n",
    "predictions = make_predictions(loaded_model_raw, imdb_test)\n",
    "y_pred = predictions.argmax(axis=1)\n",
    "\n",
    "print('accuracy score:', accuracy_score(y_true, y_pred))\n",
    "print('precision score:', precision_score(y_true, y_pred))\n",
    "print('recall score:', recall_score(y_true, y_pred))\n",
    "print('f1 score:', f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Note__: `loaded_model_raw` is an instance of `BertModelWrapper`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without fine-tuning, the model is very stubborn and predicts the same class all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance: ['This movie is horrible'], predicted label: negative, predictions: [-3.139483   3.0472898]\n",
      "instance: ['This movie is great!'], predicted label: positive, predictions: [ 2.5500994 -3.031134 ]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(fields=list(fields.values()), \n",
    "                example_format='list',\n",
    "                feature_transformer=cast_to_torch_transformer,\n",
    "                model=loaded_model)\n",
    "\n",
    "instances = [\n",
    "    ['This movie is horrible'],\n",
    "    ['This movie is great!']\n",
    "]\n",
    "\n",
    "for instance in instances:\n",
    "    predictions = pipe.predict_raw(instance)\n",
    "    print(f'instance: {instance}, predicted label: '\n",
    "          f'{fields[\"label\"].vocab.itos[predictions.argmax()]}, '\n",
    "          f'predictions: {predictions}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "podium-env",
   "language": "python",
   "name": "podium-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
