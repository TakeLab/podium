{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podium installation\n",
    "! pip install podium-nlp\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/takelab/podium\n",
    "\n",
    "# Additional dependencies required to run this notebook:\n",
    "! pip install sacremoses clean-text spacy truecase https://github.com/LIAAD/yake/archive/v0.4.2.tar.gz\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podium contains a number of predefined hook classes which you can instantiate and use in your Fields. Most of these hooks (if they have the `as_pretokenization` constructor parameter) are customizable and can work both as pretokenization hooks as well as posttokenization hooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** If you apply a hook as posttokenization, it will be called for each element in the tokenized sequence!\n",
    "> \n",
    "> Hooks should be cast to posttokenization **only** if their application would otherwise influence the tokenization process. Setting a hook to posttokenization is expected to take longer than the same hook being used during pretokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moses Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MosesNormalizer` is a hook that wraps `MosesPunctNormalizer` from [sacremoses](https://github.com/alvations/sacremoses). Accepts the language for the punctuation normalizer to be applied on. Normalizes whitespace, unicode punctuations, numbers and special control characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A _very_ spaced sentence"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.preproc import MosesNormalizer\n",
    "moses = MosesNormalizer(language=\"en\")\n",
    "text = \"A                 _very_     spaced   sentence\"\n",
    "print(moses(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, MosesNormalizer is a pretokenization hook, which means it expects a single string as an argument. We can cast it to a post-tokenization hook by setting `as_pretokenization=False` in the constructor. As a result, the hook now expectes two arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, ['A ', ' _very_', ' spaced ', ' sentence'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moses = MosesNormalizer(language=\"en\", as_pretokenization=False)\n",
    "raw_text = None\n",
    "tokenized_text = [\"A        \",\"         _very_\",\"     spaced  \",\" sentence\"]\n",
    "print(moses(raw_text, tokenized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RegexReplace` is a hook that applies regex replacements. As an example, we can replace all non-alphanumeric characters from SST instances. First, we will setup loading of the SST dataset, which we will use throughout the following examples. For reference, we will now print out the instance we will apply the transformation on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'text': (None, ['A', 'slick', ',', 'engrossing', 'melodrama', '.']), 'label': (None, 'positive')})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium import Field, LabelField, Vocab\n",
    "from podium.datasets import SST\n",
    "\n",
    "text = Field('text', numericalizer=Vocab())\n",
    "label = LabelField('label')\n",
    "fields={'text':text, 'label':label}\n",
    "sst_train, sst_test, sst_dev = SST.get_dataset_splits(fields=fields)\n",
    "print(sst_train[222])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define our replacements, each a `(Pattern, str)` tuple where the pattern matched is replaced with the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'text': ('A slick  engrossing melodrama ', ['A', 'slick', 'engrossing', 'melodrama']), 'label': (None, 'positive')})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.preproc import RegexReplace\n",
    "non_alpha = r\"[^a-zA-Z\\d\\s]\"\n",
    "replacements = RegexReplace([\n",
    "    (non_alpha, '')\n",
    "])\n",
    "text = Field('text', numericalizer=Vocab(),\n",
    "             pretokenize_hooks=[replacements],\n",
    "             keep_raw=True)\n",
    "fields={'text':text, 'label':label}\n",
    "sst_train, sst_test, sst_dev = SST.get_dataset_splits(fields=fields)\n",
    "print(sst_train[222])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the non-alphanumeric characters have been removed from the sequence. Similarly, you can pass a list of regex replacements which will then be executed in the order given. Please do take note that regular expressions are not known for their speed and if you can perform a replacement without using one, it might be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextCleanUp` is a **pretokenization** hook, a wrapper of a versatile library that can perform a number of text cleaning operations. For full options, we refer the reader to the\n",
    "[cleantext](https://github.com/jfilter/clean-text) repository . In Podium, `TextCleanUp` can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'text': ('A slick engrossing melodrama', ['A', 'slick', 'engrossing', 'melodrama']), 'label': (None, 'positive')})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.preproc import TextCleanUp\n",
    "cleanup = TextCleanUp(remove_punct=True)\n",
    "text = Field('text', numericalizer=Vocab(), pretokenize_hooks=[cleanup], keep_raw=True)\n",
    "sst_train, sst_test, sst_dev = SST.get_dataset_splits(fields={'text':text, 'label':label})\n",
    "print(sst_train[222])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NLTKStemmer` is a **post-tokenization** hook that applies the NLTK stemmer to the tokenized sequence. This hook, for obvious reasons, cannot be used as a pretokenization hook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'text': (None, ['a', 'slick', ',', 'engross', 'melodrama', '.']), 'label': (None, 'positive')})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.preproc import NLTKStemmer\n",
    "stemmer = NLTKStemmer(language=\"en\", ignore_stopwords=True)\n",
    "text = Field('text', numericalizer=Vocab(), posttokenize_hooks=[stemmer])\n",
    "sst_train, sst_test, sst_dev = SST.get_dataset_splits(fields={'text':text, 'label':label})\n",
    "print(sst_train[222])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SpacyLemmatizer` is a **post-tokenization** hook that applies the Spacy lemmatizer to the tokenized sequence. This hook, for obvious reasons, cannot be used as a pretokenization hook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'text': (None, ['a', 'slick', ',', 'engross', 'melodrama', '.']), 'label': (None, 'positive')})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.preproc import SpacyLemmatizer\n",
    "lemmatizer = SpacyLemmatizer(language=\"en\")\n",
    "text = Field('text', numericalizer=Vocab(), posttokenize_hooks=[stemmer])\n",
    "sst_train, sst_test, sst_dev = SST.get_dataset_splits(fields={'text':text, 'label':label})\n",
    "print(sst_train[222])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truecase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`truecase` is a **pre-tokenization** hook that applies [truecasing](https://github.com/daltonfury42/truecase) the the input strings. The `oov` argument controls how the library handles out-of-vocabulary tokens, the options being `{\"title\", \"lower\", \"as-is\"}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hey, what is the weather in New York?"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.preproc import truecase\n",
    "apply_truecase = truecase(oov='as-is')\n",
    "print(apply_truecase('hey, what is the weather in new york?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`remove_stopwords` is a **post-tokenization** hook that removes stop words from the tokenized sequence. The list of stop words is provided by [SpaCy](https://spacy.io/) and the language is controlled by the `language` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **WARNING:** The spacy stopword list is in lowercase, so it is recommended to lowercase your tokens prior to stopword removal to avoid unexpected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, [opinion', 'exciting', 'funny', 'movie'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.preproc import remove_stopwords\n",
    "remove_stopwords_hook = remove_stopwords('en')\n",
    "raw_text = None\n",
    "tokenized_text = ['in', 'my', 'opinion', 'an', 'exciting', 'and', 'funny', 'movie']\n",
    "print(remove_stopwords_hook(raw_text, tokenized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various tools that can be used for preprocessing textual datasets, not necessarily intended to be used as hooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SpaCy sentencizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SpacySentencizer` can bse used to split input strings into sentences prior to tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yet another keyword extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YAKE` can be used to extract keywords from input strings."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
