{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podium installation\n",
    "! pip install podium-nlp\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/takelab/podium\n",
    "\n",
    "# Additional dependencies required to run this notebook:\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch RNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will cover a simple RNN-based classifier model implemented in Pytorch. We will use the IMDB dataset loaded from ðŸ¤—/datasets, preprocess it with Fields and train the model briefly.\n",
    "While having a GPU is not necessary it is recommended as otherwise training the model, even for a single epoch, will take a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a dataset from ðŸ¤—/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have covered in [Loading ðŸ¤— datasets](http://takelab.fer.hr/podium/walkthrough.html#hf-loading), we have implemented wrappers around ðŸ¤— dataset classes to enable working with the plethora of datasets implemented therein. We will now briefly go through (1) loading a dataset from ðŸ¤—/datasets and (2) wrapping it in Podium classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})\n",
       "{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None),\n",
       " 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "imdb = load_dataset('imdb')\n",
    "print(imdb)\n",
    "from pprint import pprint\n",
    "pprint(imdb['train'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling `load_dataset` the dataset was downloaded and cached on disk through the `datasets` library. The dataset has two splits we are interested in (`train` and `test`).\n",
    "The main thing we need to pay attention to are the `features` of the dataset, in this case `text` and `label`. These features, or data columns, need to be mapped to (and processed by) Podium Fields.\n",
    "\n",
    "For convenience, we have implemented automatic `Field` type inference from ðŸ¤— dataset features -- however it is far from perfect as we have to make many assumptions on the way. We will now wrap the IMDB dataset in Podium and show the automatically inferred Fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Field({\n",
       "    name: 'text',\n",
       "    keep_raw: False,\n",
       "    is_target: False,\n",
       "    vocab: Vocab({specials: ('<UNK>', '<PAD>'), eager: False, is_finalized: True, size: 280619})\n",
       "})\n",
       "LabelField({\n",
       "    name: 'label',\n",
       "    keep_raw: False,\n",
       "    is_target: True\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.datasets.hf import HFDatasetConverter as HF\n",
    "splits = HF.from_dataset_dict(imdb)\n",
    "imdb_train, imdb_test = splits['train'], splits['test']\n",
    "imdb_train.finalize_fields() # Construct the vocab\n",
    "print(*imdb_train.fields, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the Fields were constructed well, but there are a couple of drawbacks for this concrete dataset. Firstly, the size of the vocabulary is very large (`280619`) -- we would like to trim this down to a reasonable number as we won't be using subword tokenization in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({\n",
       "    text: (None, ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', '\"Teachers\".', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', \"High's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"Teachers\".', 'The', 'scramble', 'to', 'survive', 'financially,', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp,', 'the', 'pettiness', 'of', 'the', 'whole', 'situation,', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school,', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High.', 'A', 'classic', 'line:', 'INSPECTOR:', \"I'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers.', 'STUDENT:', 'Welcome', 'to', 'Bromwell', 'High.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched.', 'What', 'a', 'pity', 'that', 'it', \"isn't!\"]),\n",
       "    label: (None, 1)\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(imdb_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When inspecting a concrete instance, there are a few more things to note. Firstly, IMDB instances can be quite long (on average around 200 tokens per instance), secondly, the text wasn't tokenized properly near sentence boundaries (due to using the default `str.split` tokenizer) and lastly, the text has varying casing.\n",
    "We will instead define our own Fields for the corresponding features, add posttokenization hooks which will transform the data, and use those Fields to replace the automatically inferred ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HFDatasetConverter({\n",
       "    dataset_name: imdb,\n",
       "    size: 25000,\n",
       "    fields: [\n",
       "        Field({\n",
       "            name: 'text',\n",
       "            keep_raw: False,\n",
       "            is_target: False,\n",
       "            vocab: Vocab({specials: ('<UNK>', '<PAD>'), eager: False, is_finalized: True, size: 10000})\n",
       "        }),\n",
       "        LabelField({\n",
       "            name: 'label',\n",
       "            keep_raw: False,\n",
       "            is_target: True\n",
       "        })\n",
       "    ]\n",
       "})\n",
       "Example({\n",
       "    text: (None, ['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'teachers', '\"', '.', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'high', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'teachers', '\"', '.', 'the', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', '.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'i', 'immediately', 'recalled', '.........', 'at', '..........', 'high', '.', 'a', 'classic', 'line', ':', 'inspector', ':', 'i', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'student', ':', 'welcome', 'to', 'bromwell', 'high', '.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', '.', 'what', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!']),\n",
       "    label: (None, 1)\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium import Field, LabelField, Vocab\n",
    "\n",
    "# Lowercasing as a post-tokenization hook\n",
    "def lowercase(raw, tokenized):\n",
    "  return raw, [token.lower() for token in tokenized]\n",
    "\n",
    "# Truncating as a post-tokenization hook\n",
    "def truncate(raw, tokenized, max_length=200):\n",
    "    return raw, tokenized[:max_length]\n",
    "\n",
    "vocab = Vocab(max_size=10000)\n",
    "text = Field(name=\"text\", \n",
    "             numericalizer=vocab,\n",
    "             include_lengths=True,\n",
    "             tokenizer=\"spacy-en_core_web_sm\",\n",
    "             posttokenize_hooks=[truncate, lowercase])\n",
    "\n",
    "# The labels are already mapped to indices in /datasets so we will\n",
    "# pass them through\n",
    "label = LabelField(name=\"label\", numericalizer=lambda x: x)\n",
    "fields = {\n",
    "    'text': text,\n",
    "    'label': label\n",
    "}\n",
    "\n",
    "# Use the given Fields to load the dataset again\n",
    "splits = HF.from_dataset_dict(imdb, fields=fields)\n",
    "imdb_train, imdb_test = splits['train'], splits['test']\n",
    "imdb_train.finalize_fields()\n",
    "print(imdb_train)\n",
    "print(imdb_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the effect of our hooks and using the spacy tokenizer. Now our dataset will be a bit cleaner to work with. Some data cleaning would still be desired, such as removing tokens which only contain punctuation, but we leave this exercise to the reader :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most use-cases, we want to use pre-trained word embeddings along with our neural model. With Podium, this process is very simple. If your field uses a vocabulary, it has already built an inventory of tokens for your dataset.\n",
    "\n",
    "For example, we will use the [GloVe](https://nlp.stanford.edu/projects/glove/) vectors. You can read more about loading pretrained vectors in [Loading pretrained word vectors](http://takelab.fer.hr/podium/walkthrough.html#pretrained), but the procedure to load these vectors has two steps: (1) initialize the vector class, which sets all the required paths and (2) obtain the vectors for a pre-defined list of words by calling `load_vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For vocabulary of size: 10000 loaded embedding matrix of shape: (10000, 300)\n",
       "Vector for sport: [ 0.34566    0.15934    0.48444   -0.13693    0.18737    0.2678"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.vectorizers import GloVe\n",
    "vocab = fields['text'].vocab\n",
    "glove = GloVe()\n",
    "embeddings = glove.load_vocab(vocab)\n",
    "print(f\"For vocabulary of size: {len(vocab)} loaded embedding matrix of shape: {embeddings.shape}\")\n",
    "# We can obtain vectors for a single word (given the word is loaded) like this:\n",
    "word = \"sport\"\n",
    "print(f\"Vector for {word}: {glove.token_to_vector(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -0.39159    0.4931    -0.76111   -1.4586     0.41475    0.55837\n",
    "  ...\n",
    "  0.13802    0.36619    0.19734    0.35701   -0.42228   -0.25242\n",
    " -0.050651  -0.041129   0.15092    0.22084    0.52252   -0.27224  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a simple neural model in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement a very simple neural classification model -- a 2-layer BiGRU with a single hidden layer classifier on top of its last hidden state. Many improvements to the model can be made, but this is not our current focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding, embed_dim=300, hidden_dim=300, num_labels=2):\n",
    "      super(NLIModel, self).__init__()\n",
    "      self.embedding = embedding\n",
    "      self.encoder = nn.GRU(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3\n",
    "      )\n",
    "      self.decoder = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, num_labels)\n",
    "      )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        e = self.embedding(x)\n",
    "        h_pack = pack_padded_sequence(e, \n",
    "                                      lengths,\n",
    "                                      enforce_sorted=False,\n",
    "                                      batch_first=True)\n",
    "\n",
    "        _, h = self.encoder(h_pack) # [2L x B x H]\n",
    "\n",
    "        # Concat last state of left and right directions\n",
    "        h = torch.cat([h[-1], h[-2]], dim=-1) # [B x 2H]\n",
    "        return self.decoder(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the prerequisites for pytorch model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "padding_index = text.vocab.get_padding_index()\n",
    "embedding_matrix = nn.Embedding(len(text.vocab), embed_dim,\n",
    "                                padding_idx=padding_index)\n",
    "# Copy the pretrained GloVe word embeddings\n",
    "embedding_matrix.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNNClassifier(embedding_matrix)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model setup code ready, we will first define helper method to measure accuracy of our model after each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def update_stats(accuracy, confusion_matrix, logits, y):\n",
    "    _, max_ind = torch.max(logits, 1)\n",
    "    equal = torch.eq(max_ind, y)\n",
    "    correct = int(torch.sum(equal))\n",
    "\n",
    "    for j, i in zip(max_ind, y):\n",
    "        confusion_matrix[int(i),int(j)]+=1\n",
    "    return accuracy + correct, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now the training loop for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def train(model, data, optimizer, criterion, num_labels):\n",
    "    model.train()\n",
    "    accuracy, confusion_matrix = 0, np.zeros((num_labels, num_labels), dtype=int)\n",
    "    for batch_num, batch in tqdm.tqdm(enumerate(data), total=len(data)):\n",
    "        x, lens = batch.text\n",
    "        y = batch.label\n",
    "        logits = model(x, lens)\n",
    "        accuracy, confusion_matrix = update_stats(accuracy, confusion_matrix, logits, y)\n",
    "        loss = criterion(logits, y.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"[Accuracy]: {}/{} : {:.3f}%\".format(\n",
    "          accuracy, len(data)*data.batch_size, accuracy / len(data) / data.batch_size * 100))\n",
    "    return accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now, we are done with our model code. Let's turn back to Podium and see how we can set up batching for our training loop to start ticking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatching data in Podium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered batching data in [Minibatching data](http://takelab.fer.hr/podium/quickstart.html#minibatching) and advanced batching through bucketing in [Bucketing instances when iterating](http://takelab.fer.hr/podium/advanced.html#bucketing). We will use the plain Iterator and leave bucketing for you to change to see how much the model speeds up when minimizing padding. One change we would like to do when iterating over data is to obtain the data matrices as torch tensors on the `device` we defined previously. We will now demonstrate how to do this by setting the `matrix_class` argument of the `Iterator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Accuracy]: 20050/25024 : 80.123%\n",
       "[Accuracy]: 22683/25024 : 90.645%\n",
       "[Accuracy]: 23709/25024 : 94.745%\n",
       "[Accuracy]: 24323/25024 : 97.199%\n",
       "[Accuracy]: 24595/25024 : 98.286%"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium import Iterator\n",
    "# Closure for converting data to given device\n",
    "def device_tensor(data):\n",
    "    return torch.tensor(data).to(device)\n",
    "# Initialize our iterator\n",
    "train_iter = Iterator(imdb_train, batch_size=32, matrix_class=device_tensor)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(model, train_iter, optimizer, criterion, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done! In our case, the model takes about one minute per epoch on a GPU, but this can be sped up by using bucketing, which we recommend you try out yourself."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
