{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podium installation\n",
    "! pip install podium-nlp\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/TakeLab/podium.git\n",
    "\n",
    "# Additional dependencies required to run this notebook:\n",
    "! pip install datasets spacy\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core component of Podium is the `Dataset` class, a shallow wrapper which contains instances of a machine learning dataset and the preprocessing pipeline for each data field. \n",
    "\n",
    "Podium datasets come in three flavors:\n",
    "\n",
    "- **Built-in datasets**: Podium contains data load and download functionality for some commonly used datasets in separate classes. See how to load built-in datasets here: [Built-in datasets](#builtin-loading).\n",
    "- **Tabular datasets**: Podium allows you to load datasets in standardized format through `TabularDataset` and `DiskBackedDataset` classes. See how to load tabular datasets here: [Loading your custom dataset](#custom-loading).\n",
    "\n",
    "  - Regular tabular datasets are memory-backed, while the arrow version is disk-backed.\n",
    "- **HuggingFace datasets**: Podium wraps the popular [ðŸ¤— datasets](https://github.com/huggingface/datasets) library and allows you to convert every ðŸ¤— dataset to a Podium dataset. See how to load ðŸ¤— datasets here: [Loading ðŸ¤— datasets](#hf-loading)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='builtin-loading' id='builtin-loading'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One built-in dataset available in Podium is the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/treebank.html). In order to load the dataset, it is enough to call the `get_dataset_splits` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SST({\n",
       "    size: 6920,\n",
       "    fields: [\n",
       "        Field({\n",
       "            name: 'text',\n",
       "            keep_raw: False,\n",
       "            is_target: False,\n",
       "            vocab: Vocab({specials: ('<UNK>', '<PAD>'), eager: False, is_finalized: True, size: 16284})\n",
       "        }),\n",
       "        LabelField({\n",
       "            name: 'label',\n",
       "            keep_raw: False,\n",
       "            is_target: True,\n",
       "            vocab: Vocab({specials: (), eager: False, is_finalized: True, size: 2})\n",
       "        })\n",
       "    ]\n",
       "})\n",
       "Example({\n",
       "    text: (None, ['A', 'slick', ',', 'engrossing', 'melodrama', '.']),\n",
       "    label: (None, 'positive')\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.datasets import SST\n",
    "sst_train, sst_dev, sst_test = SST.get_dataset_splits()\n",
    "sst_train.finalize_fields()\n",
    "print(sst_train)\n",
    "print(sst_train[222]) # A short example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each built-in Podium dataset has a `get_dataset_splits` method, which returns the *train*, *test* and *validation* splits of that dataset, if available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='hf-loading' id='hf-loading'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading ðŸ¤— datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The popular [ðŸ¤— datasets](https://github.com/huggingface/datasets) library implements a large number of NLP datasets. For simplicity, we have created a wrapper for ðŸ¤— datasets, which allows you to map all of the 600+ datasets directly to your Podium pipeline.\n",
    "\n",
    "Converting a dataset from ðŸ¤— datasets into Podium requires some work from your side, although we have automated it as much as possible. We will first take a look at one example ðŸ¤— dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'unsupervised'])\n",
       "{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None),\n",
       " 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Loading a huggingface dataset returns an instance of DatasetDict\n",
    "# which contains the dataset splits (usually: train, valid, test) \n",
    "imdb = load_dataset('imdb')\n",
    "\n",
    "print(imdb.keys())\n",
    "\n",
    "# Each dataset has a set of features which need to be mapped\n",
    "# to Podium Fields.\n",
    "pprint(imdb['train'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is the case with loading your custom dataset, `features` of ðŸ¤— datasets need to be mapped to Podium Fields in order to direct the data flow for preprocessing.\n",
    "\n",
    "Datasets from ðŸ¤— need to either (1) be wrapped them in `HFDatasetConverter`, in which case they remain as [pyarrow](https://arrow.apache.org/docs/python/) disk-backed datasets or (2) cast into a Podium `Dataset`, making them concrete and loading them in memory. The latter operation can be memory intensive for some datasets. We will first take a look at using disk-backed ðŸ¤— datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Field({\n",
       "      name: 'text',\n",
       "      keep_raw: False,\n",
       "      is_target: False,\n",
       "      vocab: Vocab({specials: ('<UNK>', '<PAD>'), eager: False, is_finalized: True, size: 280619})\n",
       "  }), 'label': LabelField({\n",
       "      name: 'label',\n",
       "      keep_raw: False,\n",
       "      is_target: True\n",
       "})}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.datasets.hf import HFDatasetConverter as HF\n",
    "# We create an adapter for huggingface dataset schema to podium Fields,\n",
    "# allowing you to use wrapped ðŸ¤— datasets as Podium ones\n",
    "imdb_train, imdb_test, imdb_unsupervised = HF.from_dataset_dict(imdb).values()\n",
    "imdb_train.finalize_fields()\n",
    "\n",
    "print(imdb_train.field_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Conversion from features to Fields is **automatically inferred** by default. This is a process which can be error prone, many assumptions have to be made. Nevertheless, it will work for basic use-cases.\n",
    "> In general, we recommend you set the `fields` argument of `from_dataset_dict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load a ðŸ¤— dataset, we internally perform automatic Field type inference and create Fields. While we expect these Fields to work in most cases, we recommend you try constructing your own.\n",
    "\n",
    "Once the `Field` s are constructed, we can use the dataset as if it was part of Podium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[    49     24      7    172   1671    156     22  11976      5   1757\n",
       "  3409   7124    202      ...     1]\n",
       "[   523     64     28    353     10      3    227     21      7  73941\n",
       "    52     28    186    ...  8668]]\n",
       "[[0]\n",
       " [0]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium import Iterator\n",
    "it = Iterator(imdb_train, batch_size=2)\n",
    "\n",
    "text, label = next(iter(it))\n",
    "print(text, label, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='custom-loading' id='custom-loading'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading your custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered loading built-in datasets. However, it is often the case that you want to work on a dataset that you either constructed or we have not yet implemented the loading function for. If that dataset is in a simple tabular format (one row = one instance), you can use `TabularDataset`.\n",
    "\n",
    "Let's take an example of a natural language inference (NLI) dataset. In NLI, datasets have two input fields: the *premise* and the *hypothesis* and a single, multi-class label. The first two rows of such a dataset written in comma-separated-values (*csv*) format could look as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "premise,hypothesis,label\n",
    "A man inspects the uniform of a figure in some East Asian country.,The man is sleeping,contradiction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "dataset_path = 'my_dataset.csv'\n",
    "field_names = ('premise', 'hypothesis', 'label')\n",
    "with open(dataset_path, 'w', newline='') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\n",
    "        'premise': 'A man inspects the uniform of a figure in some East Asian country .',\n",
    "        'hypothesis': 'The man is sleeping ',\n",
    "        'label': 'contradiction',\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, we need to define three Fields. We also might want the fields for *premise* and *hypothesis* to share their Vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularDataset({\n",
       "    size: 1,\n",
       "    fields: [\n",
       "            Field({\n",
       "                name: 'premise',\n",
       "                keep_raw: False,\n",
       "                is_target: False,\n",
       "                vocab: Vocab({specials: ('<UNK>', '<PAD>'), eager: False, is_finalized: True, size: 19})\n",
       "            }),\n",
       "            Field({\n",
       "                name: 'hypothesis',\n",
       "                keep_raw: False,\n",
       "                is_target: False,\n",
       "                vocab: Vocab({specials: ('<UNK>', '<PAD>'), eager: False, is_finalized: True, size: 19})\n",
       "            }),\n",
       "            LabelField({\n",
       "                name: 'label',\n",
       "                keep_raw: False,\n",
       "                is_target: True,\n",
       "                vocab: Vocab({specials: (), eager: False, is_finalized: True, size: 1})\n",
       "            })\n",
       "    ]\n",
       "})\n",
       "['<UNK>', '<PAD>', 'man', 'A', 'inspects', 'the', 'uniform', 'of', 'a', 'figure', 'in', 'some', 'East', 'Asian', 'country', '.', 'The', 'is', 'sleeping']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium import TabularDataset, Vocab, Field, LabelField\n",
    "shared_vocab = Vocab()\n",
    "fields = {'premise':   Field('premise', numericalizer=shared_vocab),\n",
    "          'hypothesis':Field('hypothesis', numericalizer=shared_vocab),\n",
    "          'label':     LabelField('label')}\n",
    "\n",
    "dataset = TabularDataset(dataset_path, format='csv', fields=fields)\n",
    "dataset.finalize_fields()\n",
    "print(dataset)\n",
    "print(shared_vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `TabularDataset` supports three keyword formats out-of-the-box:\n",
    "\n",
    "1. **csv**: the comma-separated values format, which uses python's `csv.reader` to read comma delimited files. Pass additional arguments to the reader via the `csv_reader_params` argument,\n",
    "2. **tsv**: the tab-separated values format, handled similarly to csv except that the delimiter is `\"\\t\"`,\n",
    "3. **json**: the line-json format, where each line of the input file in in json format.\n",
    "\n",
    "Since these formats are not exhaustive, we also support loading other custom line-dataset formats through using the `line2example` argument of `TabularDataset`.\n",
    "The `line2example` function should accept a single line of the dataset file as its argument and output a sequence of input data which will be mapped to the Fields. An example definition of a function which splits a csv dataset line into its components is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({\n",
       "    premise: (None, ['A', 'man', 'inspects', 'the', 'uniform', 'of', 'a', 'figure', 'in', 'some', 'East', 'Asian', 'country', '.']),\n",
       "    hypothesis: (None, ['The', 'man', 'is', 'sleeping']),\n",
       "    label: (None, 'contradiction')\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_split(line):\n",
    "    line_parts = line.strip().split(\",\")\n",
    "    return line_parts\n",
    "\n",
    "dataset = TabularDataset(dataset_path, fields=fields, line2example=custom_split)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for simplicity, we (naively) assume that the content of the Field data will not contain commas. \n",
    "Please note that the line which we pass to the `line2example` function will contain the newline symbol which you need to strip.\n",
    "\n",
    "When the `line2example` argument is not `None`, the `format` argument will be ignored.\n",
    "\n",
    "In addition to datasets in the standard tabular format, we also support loading datasets from [pandas](https://pandas.pydata.org/) with `Dataset.from_pandas` or the CoNLL column-based data format `CoNLLUDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='vocab' id='vocab'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that our dataset has two Fields: text and label. We will go into detail on what exactly Fields are later, but for now let's just retrieve and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Field({\n",
       "    name: 'text',\n",
       "    keep_raw: False,\n",
       "    is_target: False,\n",
       "    vocab: Vocab({specials: ('<UNK>', '<PAD>'), eager: False, is_finalized: True, size: 16284})\n",
       "})\n",
       "LabelField({\n",
       "    name: 'label',\n",
       "    keep_raw: False,\n",
       "    is_target: True,\n",
       "    vocab: Vocab({specials: (), eager: False, is_finalized: True, size: 2})\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field, label_field = sst_train.fields\n",
    "print(text_field, label_field, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside each of these two fields we can see a `Vocab` class, used for numericalization (converting tokens to indices). A Vocab is defined by two maps: the string-to-index mapping :attr:*podium.Vocab.stoi* and the index-to-string mapping :attr:*podium.Vocab.itos*.\n",
    "\n",
    "After loading all the datasets you wish to build your vocabularies on, you need to call the `Dataset.finalize_fields()` method to signal that the vocabularies should be constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='finalizing_vocab' id='finalizing_vocab'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalizing vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now briefly explain the reasoning behind the required boilerplate `finalize_fields` call and why it is important. The main reason is that manually calling this line gives users more control over which dataset splits, or datasets, are the vocabularies constructed.\n",
    "\n",
    "For an example, we might want to either construct the vocabulary on **all** dataset splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab({specials: ('<UNK>', '<PAD>'), eager: False, is_finalized: True, size: 19425})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, dev, test = SST.get_dataset_splits()\n",
    "train.finalize_fields(train, dev, test)\n",
    "print(train.field('text').vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did this by passing any number of Datasets as the argument of `finalize_fields`, indicating  the frequencies for the Vocabularies should be counted on all of those datasets. Once `finalize_fields` is called on a `Dataset` instance, the `Dataset` iterates over all of its `Fields`, updates frequency counts of their `Vocab` instances (if they are used) on all given datasets.\n",
    "\n",
    "In case the argument is left as `None` (default), the vocabularies will only be built on the dataset on which `finalize_fields` is called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab({specials: ('<UNK>', '<PAD>'), eager: False, is_finalized: True, size: 16284})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, dev, test = SST.get_dataset_splits()\n",
    "train.finalize_fields()\n",
    "print(train.field('text').vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the `Vocab` was not built in the `dev` and `test` splits, preventing information leakage in some types of models. Another case where manual finalization of Fields is useful is [Dataset concatenation](http://takelab.fer.hr/podium/advanced.html#dataset_concat). All in all, this line of boilerplate code allows a higher degree of control to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** If your Dataset doesn't use a Podium `Vocab`, you are not required to call `finalize_fields`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing Vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can customize Podium Vocabularies in one of two ways -- by controlling their constructor parameters and by defining a Vocabulary manually. For the latter approach, the `Vocab` class has two static constructors: `Vocab.from_itos` and `Vocab.from_stoi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab({specials: (), eager: False, is_finalized: True, size: 4})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium import Vocab\n",
    "custom_stoi = {'This':0, 'is':1, 'a':2, 'sample':3}\n",
    "vocab = Vocab.from_stoi(custom_stoi)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we can define a static dictionary which we might have obtained on another dataset to use for our current task. Similarly, it is possible to define a `Vocab` by a sequence of strings -- an `itos`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab({specials: ('<UNK>',), eager: False, is_finalized: True, size: 5})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.vocab import UNK\n",
    "custom_itos = [UNK(), 'this', 'is', 'a', 'sample']\n",
    "vocab = Vocab.from_itos(custom_itos)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we have also defined a Special token ([Special tokens](http://takelab.fer.hr/podium/advanced.html#specials)) to use in our vocabulary. Both of these static constructors are equivalent and can produce the same `Vocab` mapping.\n",
    "\n",
    "We will now take a look at controlling Vocabs through their constructor parameters. In the previous code block we can see that the Vocab for the `text` field has a size of 16282. The Vocab by default includes all the tokens present in the dataset, whichever their frequency might be. There are two ways to control the size of your vocabulary:\n",
    "\n",
    "1. Setting the minimum frequency (inclusive) for a token to be used in a Vocab: the :attr:*podium.Vocab.min_freq* argument\n",
    "2. Setting the maximum size of the Vocab: the :attr:*podium.Vocab.max_size* argument\n",
    "\n",
    "You might want to limit the size of your Vocab for larger datasets. To do so, define your own vocabulary as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from podium import Vocab\n",
    "small_vocabulary = Vocab(max_size=5000, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this new Vocab with a dataset, we first need to get familiar with Fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='fields' id='fields'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing the preprocessing pipeline with Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing in Podium is wholly encapsulated in the flexible `Field` class. Default Fields for the SST dataset are defined in the `SST.get_dataset_splits` method, but you can easily redefine and customize them. We will only scratch the surface of customizing Fields in this section.\n",
    "\n",
    "You can think of Fields as the path your data takes from the input to your model. In order for Fields to be able to process data, you need to which input data columns will pass through which Fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_static/field_visual.png\" alt=\"Field visualisation\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the image, your job is to define the color-coding between input data columns and Fields. If the columns in your dataset are named (as they are in the SST dataset), you should define this mapping as a **dictionary** where the keys are the names of the input data columns, while the values are Fields. The name of the Field affects only the attribute where the data for that Field will be stored, and not the input column! This is due to the fact that it more complex datasets, you might want to map a single input column to multiple Fields.\n",
    "\n",
    "Fields have a number of constructor arguments, only some of which we will enumerate here:\n",
    "\n",
    "  - `name` (str): The name under which the Field's data will be stored in the dataset's Examples.\n",
    "  - `tokenizer` (str | callable | optional): The tokenizer for sequential data. You can pass a string to use a predefined tokenizer or pass a python callable which performs tokenization (e.g. a function or a class which implements `__call__`). For predefined tokenizers, you should follow the `name-args` argument formatting convention. You can use `'split'` for the `str.split` tokenizer (has no additional args) or `'spacy-en_core_web_sm'` for the spacy english tokenizer. If the data Field should not be tokenized, this argument should be None. Defaults to `'split'`.\n",
    "  - `numericalizer` (Vocab | callable | optional): The method to convert tokens to indices. Traditionally, this argument should be a Vocab instance but users can define their own numericalization function and pass it as an argument. Custom numericalization can be used when you want to ensure that a certain token has a certain index for consistency with other work. If `None`, numericalization won't be attempted.\n",
    "  - `fixed_length`: (int, optional): Usually, text batches are padded to the maximum length of an instance in batch (default behavior). However, if you are using a fixed-size model (e.g. CNN without pooling) you can use this argument to force each instance of this Field to be of `fixed_length`. Longer instances will be right-truncated, shorter instances will be padded.\n",
    "\n",
    "The SST dataset has two textual data columns (fields): (1) the input text of the movie review and (2) the label. We need to define a `Field` for each of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Field({\n",
       "    name: 'text',\n",
       "    keep_raw: False,\n",
       "    is_target: False,\n",
       "    vocab: Vocab({specials: ('<UNK>', '<PAD>'), eager: False, is_finalized: False, size: 0})\n",
       "})\n",
       "LabelField({\n",
       "    name: 'label',\n",
       "    keep_raw: False,\n",
       "    is_target: True,\n",
       "    vocab: Vocab({specials: (), eager: False, is_finalized: False, size: 0})\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium import Field, LabelField\n",
    "text = Field(name='text', numericalizer=small_vocabulary)\n",
    "label = LabelField(name='label')\n",
    "print(text, label, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have defined our Fields. In order for them to be initialized, we need to *show* them a dataset. For built-in datasets, this is done behind the scenes in the `get_dataset_splits` method. We will elaborate how to do this yourself in [Loading your custom dataset](#custom-loading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab({specials: ('<UNK>', '<PAD>'), eager: False, is_finalized: True, size: 5000})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = {'text': text, 'label': label}\n",
    "sst_train, sst_dev, sst_test = SST.get_dataset_splits(fields=fields)\n",
    "sst_train.finalize_fields()\n",
    "print(small_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new Vocab has been limited to the 5000 most frequent words. If your *Vocab* contains the unknown special token `UNK`, the words not present in the vocabulary will be set to the value of the unknown token. The unknown token is one of the default *special* tokens in the Vocab, alongside the padding token `PAD`. You can read more about these in [Special tokens](http://takelab.fer.hr/podium/advanced.html#specials).\n",
    "\n",
    "You might have noticed that we used a different type of Field: `LabelField` for the label. LabelField is one of the predefined custom Field classes with sensible default constructor arguments for its concrete use-case. We'll take a closer look at LabelFields in the following subsection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common case in datasets is a data Field which contains a label, represented as a string (e.g. positive/negative, a news document category). For defining such a Field, you would need to set a number of its arguments which would lead to a lot of repetetive code.\n",
    "\n",
    "For convenience, `LabelField` sets the required defaults for you, and all you need to define is its name. LabelFields always have a `fixed_length` of 1, are not tokenized and are by default set as the target for batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='iterating' id='iterating'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podium contains methods to iterate over data. Let's take a look at `Iterator`, the simplest data iterator. The default batch size of the iterator is *32* but we will reduce it for the sake of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch({\n",
       "      text: [[  14 1144    9 2955    8   27    4 2956 3752   10  149   62 5067   64\n",
       "           5   11   93   10  264    8   85    7 5068   72 3753   38 2048 2957\n",
       "           3 7565 3754 7566   49  778 7567    2    1]\n",
       "       [  14 2958 2420 5069    6   62   14 3755    6    4 5070   64 5071    9\n",
       "          48  830   11    7 5072    6  639   68   37 2959 2049 7568 1058  730\n",
       "          10 7569  568    6 7570 5073   10 7571    2]],\n",
       "      label: [[0]\n",
       "       [0]]\n",
       "  })"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium import Iterator\n",
    "train_iter = Iterator(sst_train, batch_size=2, shuffle=False)\n",
    "batch = next(iter(train_iter))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of things we need to unpack here. Firstly, our textual input data and class labels were converted to indices. This happened without our intervention -- built-in datasets have a default preprocessing pipeline, which handles text tokenization and numericalization.\n",
    "Secondly, while iterating we obtained a *Batch* instance. A *Batch* is a special dictionary that also acts as a *namedtuple* by supporting tuple unpacking and attribute lookup.\n",
    "\n",
    "Traditionally, when using a neural model, whether it is a RNN or a transformer variant, you require lengths of each instance in the dataset to create packed sequences or compute the attention mask, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[  14 1144    9 2955    8   27    4 2956 3752   10  149   62 5067   64\n",
       "   5   11   93   10  264    8   85    7 5068   72 3753   38 2048 2957\n",
       "   3 7565 3754 7566   49  778 7567    2    1]\n",
       " [  14 2958 2420 5069    6   62   14 3755    6    4 5070   64 5071    9\n",
       "    48  830   11    7 5072    6  639   68   37 2959 2049 7568 1058  730\n",
       "    10 7569  568    6 7570 5073   10 7571    2]]\n",
       "[36 37]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Field(name='text', numericalizer=Vocab(), include_lengths=True)\n",
    "label = LabelField(name='label')\n",
    "fields = {'text': text, 'label': label}\n",
    "sst_train, sst_dev, sst_test = SST.get_dataset_splits(fields=fields)\n",
    "sst_train.finalize_fields()\n",
    "\n",
    "train_iter = Iterator(sst_train, batch_size=2, shuffle=False)\n",
    "batch = next(iter(train_iter))\n",
    "text, lengths = batch.text\n",
    "print(text, lengths, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting the `include_lengths=True` for a Field, its batch component will be a tuple containing the numericalized batch and the lengths of each instance in the batch. When using recurrent cells, it is often the case we want to sort the instances within the batch according to length, e.g. in order for them to be used with `PackedSequence` objects.\n",
    "Since datasets can contain multiple input Fields, it is not trivial to determine which Field should be the key for the batch to be sorted. Thus, we delegate the key definition to the user, which can then be passed to the Iterator constructor via the `sort_key` parameter, as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[  14 2958 2420 5069    6   62   14 3755    6    4 5070   64 5071    9\n",
       "  48  830   11    7 5072    6  639   68   37 2959 2049 7568 1058  730\n",
       "  10 7569  568    6 7570 5073   10 7571    2]\n",
       " [  14 1144    9 2955    8   27    4 2956 3752   10  149   62 5067   64\n",
       "     5   11   93   10  264    8   85    7 5068   72 3753   38 2048 2957\n",
       "     3 7565 3754 7566   49  778 7567    2    1]]\n",
       "[37 36]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_len_sort_key(example):\n",
    "    # The argument is an instance of the Example class,\n",
    "    # containing a tuple of raw and tokenized data under\n",
    "    # the key for each Field.\n",
    "    tokens = example[\"text\"][1]\n",
    "    return -len(tokens)\n",
    "train_iter = Iterator(sst_train, batch_size=2, shuffle=False, sort_key=text_len_sort_key)\n",
    "batch = next(iter(train_iter))\n",
    "text, lengths = batch.text\n",
    "print(text, lengths, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we can see, that even for our small, two-instance batch, the elements in the batch are now properly sorted according to length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With most deep learning models, we want to use pre-trained word embeddings. In Podium, this process is very simple. If your field uses a vocabulary, it has already built an inventory of tokens for your dataset.\n",
    "\n",
    "A number of predefined vectorizers are available (`GloVe` and `NlplVectorizer`), as well as a standardized loader `BasicVectorStorage` for loading word2vec-style format of word embeddings from disk.\n",
    "\n",
    "For example, we will use the [GloVe](https://nlp.stanford.edu/projects/glove/) vectors. The procedure to load these vectors has two steps:\n",
    "\n",
    "1. Initialize the vector class, which sets all the required paths.\n",
    "   The vectors are not yet loaded from disk as you usually don't want to load the full file in memory.\n",
    "2. Obtain vectors for a pre-defined list of words by calling `load_vocab`.\n",
    "   The argument can be a `Vocab` object (which is itself an *iterable* of strings), or any sequence of strings.\n",
    "\n",
    "The output of the function call is a numpy matrix of word embeddings which you can then pass to your model to initialize the embedding matrix or to be used otherwise. The word embeddings are in the same order as the tokens in the Vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For vocabulary of size: 21701 loaded embedding matrix of shape: (16284, 300)\n",
       "Vector for sport: [ 0.34566    0.15934    0.48444   -0.13693    0.18737    0.2678\n",
       " -0.39159    0.4931    -0.76111   -1.4586     0.41475    0.55837\n",
       " ...\n",
       " -0.050651  -0.041129   0.15092    0.22084    0.52252   -0.27224  ]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podium.vectorizers import GloVe\n",
    "vocab = fields['text'].vocab\n",
    "glove = GloVe()\n",
    "embeddings = glove.load_vocab(vocab)\n",
    "print(f\"For vocabulary of size: {len(vocab)} loaded embedding matrix of shape: {embeddings.shape}\")\n",
    "# We can obtain vectors for a single word (given the word is loaded) like this:\n",
    "word = \"sport\"\n",
    "print(f\"Vector for {word}: {glove.token_to_vector(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF or count vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case you wish to use a standard shallow model, Podium also supports TF-IDF or count vectorization. We'll now briefly demonstrate how to obtain a TF-IDF matrix for your dataset. We will first load the SST dataset with a limited size Vocab in order to not blow up our RAM. \n",
    "\n",
    "As we intend to use the whole dataset at once, we will also set `disable_batch_matrix=True` in the constructor for the text Field. This option will return our dataset as a list of numericalized instances during batching instead of a numpy matrix. The benefit here is that if returned as a numpy matrix, all of the instances have to be padded, using up a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from podium.datasets import SST\n",
    "from podium import Vocab, Field, LabelField\n",
    "vocab = Vocab(max_size=5000)\n",
    "text = Field(name='text', numericalizer=vocab, disable_batch_matrix=True)\n",
    "label = LabelField(name='label')\n",
    "fields = {'text': text, 'label': label}\n",
    "sst_train, sst_dev, sst_test = SST.get_dataset_splits(fields=fields)\n",
    "sst_train.finalize_fields()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Tf-Idf vectorizer needs information from the dataset to compute the inverse document frequency, we first need to fit it on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from podium.vectorizers.tfidf import TfIdfVectorizer\n",
    "tfidf_vectorizer = TfIdfVectorizer()\n",
    "tfidf_vectorizer.fit(dataset=sst_train, field=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our vectorizer has seen the dataset as well as the vocabulary and has all the required information to compute Tf-Idf value for each instance. As is standard in using shallow models, we want to convert all of the instances in a dataset to a Tf-Idf matrix which can then be used with a support vector machine (SVM) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'scipy.sparse.csr.csr_matrix'> (6920, 4998)\n",
       "  (0, 2111) 0.617113703893198\n",
       "  (0, 549)  0.5208201737884445\n",
       "  (0, 499)  0.5116152860290002\n",
       "  (0, 19) 0.2515101839877878\n",
       "  (0, 1)  0.12681755258500052\n",
       "  (0, 0)  0.08262419651916046"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the whole dataset as a batch\n",
    "dataset_batch = sst_train.batch()\n",
    "tfidf_batch = tfidf_vectorizer.transform(dataset_batch.text)\n",
    "\n",
    "print(type(tfidf_batch), tfidf_batch.shape)\n",
    "print(tfidf_batch[222])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tf-Idf counts are highly sparse since not all words from the vocabulary are present in every instance. To reduce the memory footprint of count-based numericalization, we store the values in a [SciPy](https://www.scipy.org/) [sparse matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix), which can be used in various [scikit-learn](https://scikit-learn.org/stable/) models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
