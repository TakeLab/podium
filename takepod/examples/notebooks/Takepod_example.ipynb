{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import takepod\n",
    "import pickle\n",
    "from takepod.datasets import BucketIterator, Iterator, BasicSupervisedImdbDataset\n",
    "from takepod.storage import Field, Vocab\n",
    "from takepod.storage.vectorizers.impl import GloVe\n",
    "from takepod.models import Experiment, AbstractSupervisedModel\n",
    "from takepod.pipeline import Pipeline\n",
    "from takepod.models.trainer import AbstractTrainer\n",
    "\n",
    "def lowercase(raw, data):\n",
    "    return raw, [d.lower() for d in data]\n",
    "\n",
    "def max_length(raw, data, length=200):\n",
    "    return raw, data[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = BasicSupervisedImdbDataset.get_default_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fields():\n",
    "    # Define the vocabulary\n",
    "    vocab = Vocab(max_size=10000, min_freq=5)\n",
    "\n",
    "    text = Field(name='text', vocab=vocab, tokenizer='spacy', store_as_raw=False)\n",
    "    # Add preprpocessing hooks to model\n",
    "    # 1. Lowercase\n",
    "    text.add_posttokenize_hook(lowercase)\n",
    "    text.add_posttokenize_hook(max_length)\n",
    "\n",
    "    # Improve readability: LabelField\n",
    "    label = Field(name='label', vocab=Vocab(specials=()), is_target=True, tokenize=False)\n",
    "    return {text.name : text, label.name: label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = create_fields()\n",
    "imdb_train, imdb_test = BasicSupervisedImdbDataset.get_train_test_dataset(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct vectoziter based on vocab\n",
    "vocab = fields['text'].vocab\n",
    "vectorizer = GloVe()\n",
    "vectorizer.load_vocab(vocab)\n",
    "embeddings = vectorizer.get_embedding_matrix(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.04656    0.21318   -0.0074364 ...  0.0090611 -0.20989    0.053913 ]\n",
      " ...\n",
      " [-0.24734    0.019346   0.13974   ...  0.34035    0.0824     0.38554  ]\n",
      " [ 0.67287   -0.43249    0.1106    ... -0.16644    0.21169    0.45995  ]\n",
      " [ 0.034368   0.22004    0.14626   ... -0.18641   -0.032439   0.24544  ]]\n"
     ]
    }
   ],
   "source": [
    "# Works on simplify vectorizer branch\n",
    "embeddings = GloVe().load_vocab(vocab)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = Iterator(dataset=imdb_train, batch_size=32)\n",
    "valid_iterator = Iterator(dataset=imdb_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "RNNS = ['LSTM', 'GRU']\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, nlayers=1, dropout=0.,\n",
    "                   bidirectional=True, rnn_type='GRU'):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        assert rnn_type in RNNS, 'Use one of the following: {}'.format(str(RNNS))\n",
    "        rnn_cell = getattr(nn, rnn_type) # fetch constructor from torch.nn, cleaner than if\n",
    "        self.rnn = rnn_cell(embedding_dim, hidden_dim, nlayers, \n",
    "                                dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.rnn(input, hidden)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scale = 1. / math.sqrt(query_dim)\n",
    "\n",
    "    def forward(self, query, keys, values):\n",
    "        # Query = [BxQ]\n",
    "        # Keys = [TxBxK]\n",
    "        # Values = [TxBxV]\n",
    "        # Outputs = a:[TxB], lin_comb:[BxV]\n",
    "\n",
    "        # Here we assume q_dim == k_dim (dot product attention)\n",
    "\n",
    "        query = query.unsqueeze(1) # [BxQ] -> [Bx1xQ]\n",
    "        keys = keys.transpose(0,1).transpose(1,2) # [TxBxK] -> [BxKxT]\n",
    "        energy = torch.bmm(query, keys) # [Bx1xQ]x[BxKxT] -> [Bx1xT]\n",
    "        energy = F.softmax(energy.mul_(self.scale), dim=2) # scale, normalize\n",
    "\n",
    "        values = values.transpose(0,1) # [TxBxV] -> [BxTxV]\n",
    "        linear_combination = torch.bmm(energy, values).squeeze(1) #[Bx1xT]x[BxTxV] -> [BxV]\n",
    "        return energy, linear_combination\n",
    "\n",
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.config = cfg\n",
    "        self.embedding = nn.Embedding(cfg.vocab_size, cfg.embed_dim)\n",
    "        self.encoder = Encoder(cfg.embed_dim, cfg.hidden_dim, cfg.nlayers, \n",
    "                               cfg.dropout, cfg.bidirectional, cfg.rnn_type)\n",
    "        attention_dim = cfg.hidden_dim if not cfg.bidirectional else 2 * cfg.hidden_dim\n",
    "        self.attention = Attention(attention_dim, attention_dim, attention_dim)\n",
    "        self.decoder = nn.Linear(attention_dim, cfg.num_classes)\n",
    "\n",
    "        size = 0\n",
    "        for p in self.parameters():\n",
    "            size += p.nelement()\n",
    "        print('Total param size: {}'.format(size))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        outputs, hidden = self.encoder(self.embedding(input))\n",
    "        if isinstance(hidden, tuple): # LSTM\n",
    "            hidden = hidden[1] # take the cell state\n",
    "\n",
    "        if self.encoder.bidirectional: # need to concat the last 2 hidden layers\n",
    "            hidden = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "\n",
    "        energy, linear_combination = self.attention(hidden, outputs, outputs) \n",
    "        logits = self.decoder(linear_combination)\n",
    "        return_dict = {\n",
    "            'pred': logits,\n",
    "            'attention_weights':energy\n",
    "        }\n",
    "\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyTorchModel(AbstractSupervisedModel):\n",
    "    def __init__(self, model_class, config, criterion, optimizer, device='cpu'):\n",
    "        self.model_class = model_class\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self._model = model_class(config).to(device)\n",
    "        self.optimizer_class = optimizer\n",
    "        self.optimizer = optimizer(self.model.parameters(), config.lr)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        # This is a _step_ in the iteration process.\n",
    "        # Should assume model is in training mode\n",
    "        \n",
    "        # Train-specific code\n",
    "        self.model.train()\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        return_dict = self(X)\n",
    "        logits = return_dict['pred']\n",
    "        #print(logits.view(-1, self.config.num_classes), y.squeeze())\n",
    "        loss = self.criterion(logits.view(-1, self.config.num_classes), y.squeeze())\n",
    "        return_dict['loss'] = loss\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip)\n",
    "        self.optimizer.step()\n",
    "        return return_dict\n",
    "\n",
    "    def predict(self, X, return_as_numpy=True, **kwargs):\n",
    "        # Assumes that the model is in _eval_ mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return_dict = self(X)\n",
    "\n",
    "            if return_as_numpy:\n",
    "                #print(\"Casting to numpy\")\n",
    "                # Cast everything to numpy\n",
    "                preds = return_dict['pred']\n",
    "                preds = preds.cpu().numpy()\n",
    "                return_dict['pred'] = preds\n",
    "\n",
    "            return return_dict\n",
    "    \n",
    "    def evaluate(self, X, y, **kwargs):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return_dict = self(X)\n",
    "            logits = return_dict['pred']\n",
    "            loss = self.criterion(logits.view(-1, self.config.num_classes), y.squeeze())\n",
    "            return_dict['loss'] = loss\n",
    "            return return_dict\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # Restart model\n",
    "        self._model = self.model_class(self.config).to(self.config.device)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        print(\"Restoring model from state\")\n",
    "        self.model_class = state['model_class']\n",
    "        self.config = Config(state['config'])\n",
    "        self.device = state['device']\n",
    "        # Deserialize model\n",
    "        model = self.model_class(self.config)\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        self._model = model\n",
    "\n",
    "        # Deserialize optimizer\n",
    "        self.optimizer_class = state['optimizer_class']\n",
    "        self.optimizer = self.optimizer_class(self.model.parameters(), self.config.lr)\n",
    "        self.optimizer.load_state_dict(state['optimizer_state'])\n",
    "\n",
    "        # Deserialize loss\n",
    "        loss_class = state['loss_class']\n",
    "        self.criterion = loss_class()\n",
    "        self.criterion.load_state_dict(state['loss_state'])\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = {\n",
    "            'model_class': self.model_class,\n",
    "            'config': dict(self.config),\n",
    "            'model_state': self.model.state_dict(),\n",
    "            'optimizer_class': self.optimizer_class,\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'loss_class': self.criterion.__class__,\n",
    "            'loss_state': self.criterion.state_dict(),\n",
    "            'device': self.device\n",
    "        }\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchTrainer(AbstractTrainer):\n",
    "    def __init__(self, num_epochs, device, iterator, valid_data=None):\n",
    "        self.epochs = num_epochs\n",
    "        self.valid_data = valid_data\n",
    "        self.device = device\n",
    "        self.iterator = iterator\n",
    "\n",
    "    def train(self,\n",
    "              model,\n",
    "              dataset,\n",
    "              feature_transformer,\n",
    "              label_transform_fun,\n",
    "              **kwargs):\n",
    "        # Actual training loop\n",
    "        # Single training epoch\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            total_time = time.time()\n",
    "            for batch_num, (batch_x, batch_y) in enumerate(self.iterator(dataset)):\n",
    "                t = time.time()\n",
    "                X = torch.from_numpy(\n",
    "                    feature_transformer.transform(batch_x).swapaxes(0,1) # swap batch_size and T\n",
    "                    ).to(self.device)\n",
    "                y = torch.from_numpy(\n",
    "                    label_transform_fun(batch_y)\n",
    "                    ).to(self.device)\n",
    "\n",
    "                return_dict = model.fit(X, y)\n",
    "\n",
    "                print(\"[Batch]: {} in {:.5f} seconds, loss={:.5f}\".format(\n",
    "                       batch_num, time.time() - t, return_dict['loss']), \n",
    "                       end='\\r', flush=True)\n",
    "\n",
    "            print(f\"\\nTotal time for train epoch: {time.time() - total_time}\")\n",
    "\n",
    "            total_time = time.time()\n",
    "            for batch_num, (batch_x, batch_y) in enumerate(self.iterator(self.valid_data)):\n",
    "                t = time.time()\n",
    "                X = torch.from_numpy(\n",
    "                    feature_transformer.transform(batch_x).swapaxes(0,1) # swap batch_size and T\n",
    "                    ).to(self.device)\n",
    "                y = torch.from_numpy(\n",
    "                    label_transform_fun(batch_y)\n",
    "                    ).to(self.device)\n",
    "\n",
    "                return_dict = model.evaluate(X, y)\n",
    "                loss = return_dict['loss']\n",
    "                print(\"[Valid]: {} in {:.5f} seconds, loss={:.5f}\".format(\n",
    "                       batch_num, time.time() - t, loss), \n",
    "                       end='\\r', flush=True)\n",
    "\n",
    "            print(f\"\\nTotal time for valid epoch: {time.time() - total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\n",
      "{}\n",
      "{'a': 1}\n"
     ]
    }
   ],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, *args, **kwargs): \n",
    "        dict.__init__(self, *args, **kwargs)     \n",
    "            \n",
    "    def __getattr__(self, key):\n",
    "        #print(key)\n",
    "        return self[key]\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        #print(key, value)\n",
    "        self[key] = value\n",
    "\n",
    "c = Config({'a':1})\n",
    "print(c.a)\n",
    "print(dir(c))\n",
    "print(c.__dict__)\n",
    "print(dict(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "label_vocab = fields['label'].vocab\n",
    "# Ugly but just to check\n",
    "config_dict = {\n",
    "    'rnn_type': 'LSTM',\n",
    "    'embed_dim': 300,\n",
    "    'hidden_dim': 150,\n",
    "    'nlayers': 1,\n",
    "    'lr': 1e-3,\n",
    "    'clip': 5,\n",
    "    'epochs': 1,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.,\n",
    "    'bidirectional': True,\n",
    "    'cuda': False,\n",
    "    'vocab_size': len(vocab),\n",
    "    'num_classes': len(label_vocab),\n",
    "}\n",
    "\n",
    "device = torch.device('cpu:0')\n",
    "config = Config(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total param size: 3543002\n",
      "[Batch]: 781 in 0.34715 seconds, loss=0.56232\n",
      "Total time for train epoch: 411.41218519210815\n",
      "[Valid]: 781 in 0.04323 seconds, loss=0.65979\n",
      "Total time for valid epoch: 76.07344341278076\n"
     ]
    }
   ],
   "source": [
    "data_iterator = Iterator(batch_size=32)\n",
    "\n",
    "trainer = TorchTrainer(config.epochs, device, data_iterator, imdb_test)\n",
    "\n",
    "experiment = Experiment(MyTorchModel, trainer=trainer)\n",
    "model = experiment.fit(\n",
    "    imdb_train,\n",
    "    model_kwargs={\n",
    "        'model_class': AttentionRNN, \n",
    "        'config': config, \n",
    "        'criterion': criterion,\n",
    "        'optimizer': torch.optim.Adam,\n",
    "        'device': device\n",
    "        #'embedding': embedding\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model from state\n",
      "Total param size: 3543002\n"
     ]
    }
   ],
   "source": [
    "# Check serialization for _model_ only (should be for experiment as well)\n",
    "import pickle\n",
    "fitted_model = experiment.model\n",
    "\n",
    "model_save_file = 'model.pt'\n",
    "with open(model_save_file, 'wb') as dump_file:\n",
    "    pickle.dump(fitted_model, dump_file)\n",
    "\n",
    "with open(model_save_file, 'rb') as load_file:\n",
    "    loaded_model = pickle.load(load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For instance: ['This movie is horrible'], the prediction is: 0, with logits: [-4.701372   4.6259212]\n",
      "For instance: ['This movie is great!'], the prediction is: 1, with logits: [ 5.040716 -5.208417]\n"
     ]
    }
   ],
   "source": [
    "ft = experiment.feature_transformer\n",
    "cast_to_torch_transformer = lambda t: torch.from_numpy(ft.transform(t).swapaxes(0,1)).to(device)\n",
    "\n",
    "pipe = Pipeline(\n",
    "  fields = list(fields.values()),\n",
    "  example_format = 'list',\n",
    "  feature_transformer = cast_to_torch_transformer,\n",
    "  model = fitted_model\n",
    "  )\n",
    "\n",
    "instances = [\n",
    "        ['This movie is horrible'], \n",
    "        ['This movie is great!']\n",
    "]\n",
    "\n",
    "for instance in instances:\n",
    "    prediction = pipe.predict_raw(instance)\n",
    "    print(f\"For instance: {instance}, the prediction is: {fields['label'].vocab.itos[prediction.argmax()]}, with logits: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
