{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import takepod\n",
    "from takepod.datasets import Iterator, BasicSupervisedImdbDataset\n",
    "from takepod.storage import Field, Vocab\n",
    "from takepod.storage.vectorizers.impl import GloVe\n",
    "from takepod.models import Experiment, AbstractSupervisedModel\n",
    "from takepod.models.trainer import AbstractTrainer\n",
    "\n",
    "def lowercase(raw, data):\n",
    "    return raw, [d.lower() for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = BasicSupervisedImdbDataset.get_default_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fields():\n",
    "    # Define the vocabulary\n",
    "    vocab = Vocab(max_size=20000, min_freq=5)\n",
    "    text = Field(name='text', vocab=Vocab(), tokenizer='spacy', store_as_raw=False)\n",
    "    # Add preprpocessing hooks to model\n",
    "    # 1. Lowercase\n",
    "    text.add_posttokenize_hook(lowercase)\n",
    "        \n",
    "    # Improve readability: LabelField\n",
    "    label = Field(name='label', vocab=Vocab(specials=()), is_target=True, tokenize=False)\n",
    "    return {text.name : text, label.name: label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = create_fields()\n",
    "imdb_train, imdb_test = BasicSupervisedImdbDataset.get_train_test_dataset(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct vectoziter based on vocab\n",
    "vocab = fields['text'].vocab\n",
    "vectorizer = GloVe()\n",
    "vectorizer.load_vocab(vocab)\n",
    "embeddings = vectorizer.get_embedding_matrix(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.04656    0.21318   -0.0074364 ...  0.0090611 -0.20989    0.053913 ]\n",
      " ...\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]]\n"
     ]
    }
   ],
   "source": [
    "# Works on simplify vectorizer branch\n",
    "embeddings = GloVe().load_vocab(vocab)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = Iterator(dataset=imdb_train, batch_size=32)\n",
    "valid_iterator = Iterator(dataset=imdb_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "RNNS = ['LSTM', 'GRU']\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, nlayers=1, dropout=0.,\n",
    "                   bidirectional=True, rnn_type='GRU'):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        assert rnn_type in RNNS, 'Use one of the following: {}'.format(str(RNNS))\n",
    "        rnn_cell = getattr(nn, rnn_type) # fetch constructor from torch.nn, cleaner than if\n",
    "        self.rnn = rnn_cell(embedding_dim, hidden_dim, nlayers, \n",
    "                                dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.rnn(input, hidden)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scale = 1. / math.sqrt(query_dim)\n",
    "\n",
    "    def forward(self, query, keys, values):\n",
    "        # Query = [BxQ]\n",
    "        # Keys = [TxBxK]\n",
    "        # Values = [TxBxV]\n",
    "        # Outputs = a:[TxB], lin_comb:[BxV]\n",
    "\n",
    "        # Here we assume q_dim == k_dim (dot product attention)\n",
    "\n",
    "        query = query.unsqueeze(1) # [BxQ] -> [Bx1xQ]\n",
    "        keys = keys.transpose(0,1).transpose(1,2) # [TxBxK] -> [BxKxT]\n",
    "        energy = torch.bmm(query, keys) # [Bx1xQ]x[BxKxT] -> [Bx1xT]\n",
    "        energy = F.softmax(energy.mul_(self.scale), dim=2) # scale, normalize\n",
    "\n",
    "        values = values.transpose(0,1) # [TxBxV] -> [BxTxV]\n",
    "        linear_combination = torch.bmm(energy, values).squeeze(1) #[Bx1xT]x[BxTxV] -> [BxV]\n",
    "        return energy, linear_combination\n",
    "\n",
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.config = cfg\n",
    "        self.embedding = nn.Embedding(cfg.vocab_size, cfg.embed_dim)\n",
    "        self.encoder = Encoder(cfg.embed_dim, cfg.hidden_dim, cfg.nlayers, \n",
    "                               cfg.dropout, cfg.bidirectional, cfg.rnn_type)\n",
    "        attention_dim = cfg.hidden_dim if not cfg.bidirectional else 2 * cfg.hidden_dim\n",
    "        self.attention = Attention(attention_dim, attention_dim, attention_dim)\n",
    "        self.decoder = nn.Linear(attention_dim, cfg.num_classes)\n",
    "\n",
    "        size = 0\n",
    "        for p in self.parameters():\n",
    "            size += p.nelement()\n",
    "        print('Total param size: {}'.format(size))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        outputs, hidden = self.encoder(self.embedding(input))\n",
    "        if isinstance(hidden, tuple): # LSTM\n",
    "            hidden = hidden[1] # take the cell state\n",
    "\n",
    "        if self.encoder.bidirectional: # need to concat the last 2 hidden layers\n",
    "            hidden = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "\n",
    "        energy, linear_combination = self.attention(hidden, outputs, outputs) \n",
    "        logits = self.decoder(linear_combination)\n",
    "        meta = {'attention_weights':energy}\n",
    "\n",
    "        return logits, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchModel(AbstractSupervisedModel):\n",
    "    def __init__(self, model_class, config, criterion):\n",
    "        self.model_class = model_class\n",
    "        self.config = config\n",
    "        self._model = model_class(config)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        # This is a _step_ in the iteration process.\n",
    "        # Should assume model is in training mode\n",
    "        return_dict = self(X)\n",
    "        logits = return_dict['pred']\n",
    "        loss = self.criterion(logits.view(-1, self.config.nlabels), y.squeeze())\n",
    "        return_dict['loss'] = loss\n",
    "        return return_dict\n",
    "        \n",
    "    def predict(self, X, **kwargs):\n",
    "        # Assumes that the model is in _eval_ mode\n",
    "        return_dict = self(X)\n",
    "        return return_dict\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        # Restart model\n",
    "        self._model = self.model_class(self.config)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self._model.zero_grad()\n",
    "    \n",
    "    def set_train_mode(self):\n",
    "        self._model.train()\n",
    "    \n",
    "    def set_eval_mode(self):\n",
    "        self._model.eval()\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self._model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTrainer(AbstractTrainer):\n",
    "    def __init__(self, optimizer, num_epochs, valid_iterator=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = num_epochs\n",
    "        self.valid_iterator = valid_iterator\n",
    "    \n",
    "    def train(self,\n",
    "              model: AbstractSupervisedModel,\n",
    "              iterator: Iterator,\n",
    "              feature_transformer,\n",
    "              label_transform_fun,\n",
    "              **kwargs):\n",
    "        # Actual training loop\n",
    "        # Single training epoch\n",
    "        model.set_train_mode()\n",
    "        for batch_num, (batch_x, batch_y) in enumerate(iterator):\n",
    "            t = time.time()\n",
    "            X = torch.from_numpy(\n",
    "                feature_transformer.transform(batch_x)\n",
    "                )\n",
    "            y = torch.from_numpy(\n",
    "                label_transform_fun(batch_y)\n",
    "                )\n",
    "\n",
    "            model.zero_grad()\n",
    "            return_dict = model.fit(X, y)\n",
    "            loss = return_dict['loss']   \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self._model.parameters(), self.config.clip)\n",
    "            self.optimizer.step()\n",
    "            print(\"[Batch]: {}/{} in {:.5f} seconds\".format(\n",
    "                    batch_num, len(iterator), time.time() - t), end='\\r', flush=True)\n",
    "            \n",
    "        model.set_eval_mode()\n",
    "        with torch.no_grad():\n",
    "            for batch_num, batch_x, batch_y in enumerate(self.valid_iterator):\n",
    "                X = feature_transformer.transform(batch_x)\n",
    "                y = label_transform_fun(batch_y)\n",
    "\n",
    "                return_dict = model.fit(X, y)\n",
    "\n",
    "                loss = return_dict['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, *args, **kwargs): \n",
    "        dict.__init__(self, *args, **kwargs)     \n",
    "            \n",
    "    def __getattr__(self, key):\n",
    "        #print(key)\n",
    "        return self[key]\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        #print(key, value)\n",
    "        self[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rnn_type': 'LSTM', 'embed_dim': 300, 'hidden_dim': 300, 'nlayers': 2, 'lr': 0.001, 'clip': 5, 'epochs': 5, 'batch_size': 32, 'dropout': 0.0, 'bidirectional': True, 'cuda': False, 'vocab_size': 146592, 'num_classes': 2}\n",
      "['__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values'] {}\n",
      "146592\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "label_vocab = fields['label'].vocab\n",
    "# Ugly but just to check\n",
    "config_dict = {\n",
    "    'rnn_type': 'LSTM',\n",
    "    'embed_dim': 300,\n",
    "    'hidden_dim': 300,\n",
    "    'nlayers': 2,\n",
    "    'lr': 1e-3,\n",
    "    'clip': 5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.,\n",
    "    'bidirectional': True,\n",
    "    'cuda': False,\n",
    "    'vocab_size': len(vocab),\n",
    "    'num_classes': len(label_vocab)\n",
    "}\n",
    "\n",
    "config = Config(config_dict)\n",
    "print(config)\n",
    "print(dir(config), vars(config))\n",
    "print(config.vocab_size)\n",
    "\n",
    "#model = TorchModel(AttentionRNN, config, criterion)\n",
    "\n",
    "optimizer = optimizer = torch.optim.Adam(model.parameters(), config.lr, amsgrad=True)\n",
    "trainer = TorchTrainer(optimizer, config.epochs, valid_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total param size: 47588402\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "train_iterator = partial(Iterator, batch_size=32)\n",
    "valid_iterator = Iterator(dataset=imdb_train, batch_size=32)\n",
    "\n",
    "experiment = Experiment(TorchModel, trainer=trainer, \n",
    "                        training_iterator_callable=train_iterator)\n",
    "experiment.fit(\n",
    "    imdb_train,\n",
    "    model_kwargs={\n",
    "        'model_class': AttentionRNN, \n",
    "        'config': config, \n",
    "        'criterion': criterion\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
