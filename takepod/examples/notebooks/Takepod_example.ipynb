{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import takepod\n",
    "import pickle\n",
    "from takepod.datasets import BucketIterator, Iterator, BasicSupervisedImdbDataset\n",
    "from takepod.storage import LabelField, Field, Vocab\n",
    "from takepod.storage.vectorizers.impl import GloVe\n",
    "from takepod.models import Experiment, AbstractSupervisedModel\n",
    "from takepod.pipeline import Pipeline\n",
    "from takepod.models import AbstractTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(raw, data):\n",
    "    return raw, [d.lower() for d in data]\n",
    "\n",
    "def max_length(raw, data, length=200):\n",
    "    return raw, data[:length]\n",
    "\n",
    "def create_fields():\n",
    "    # Define the vocabulary\n",
    "    vocab = Vocab(max_size=10000, min_freq=5)\n",
    "\n",
    "    text = Field(name='text', vocab=vocab, tokenizer='spacy', store_as_raw=False)\n",
    "    # Add preprpocessing hooks to model\n",
    "    # 1. Lowercase\n",
    "    text.add_posttokenize_hook(lowercase)\n",
    "    text.add_posttokenize_hook(max_length)\n",
    "\n",
    "    # Improve readability: LabelField\n",
    "    label = LabelField(name='label', vocab = Vocab(specials=()))\n",
    "    return {text.name : text, label.name: label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = create_fields()\n",
    "# TODO: get_dataset_splits; remove Dataset, Basic, Supervised from IMDB name\n",
    "imdb_train, imdb_test = BasicSupervisedImdbDataset.get_train_test_dataset(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.04656    0.21318   -0.0074364 ...  0.0090611 -0.20989    0.053913 ]\n",
      " ...\n",
      " [-0.24734    0.019346   0.13974   ...  0.34035    0.0824     0.38554  ]\n",
      " [ 0.67287   -0.43249    0.1106    ... -0.16644    0.21169    0.45995  ]\n",
      " [ 0.034368   0.22004    0.14626   ... -0.18641   -0.032439   0.24544  ]]\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "vocab = fields['text'].vocab\n",
    "embeddings = GloVe().load_vocab(vocab)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "RNNS = ['LSTM', 'GRU']\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, nlayers=1, dropout=0.,\n",
    "                   bidirectional=True, rnn_type='GRU'):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        assert rnn_type in RNNS, 'Use one of the following: {}'.format(str(RNNS))\n",
    "        rnn_cell = getattr(nn, rnn_type) # fetch constructor from torch.nn\n",
    "        self.rnn = rnn_cell(embedding_dim, hidden_dim, nlayers, \n",
    "                                dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.rnn(input, hidden)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scale = 1. / math.sqrt(query_dim)\n",
    "\n",
    "    def forward(self, query, keys, values):\n",
    "        # Query = [BxQ]\n",
    "        # Keys = [TxBxK]\n",
    "        # Values = [TxBxV]\n",
    "        # Outputs = a:[TxB], lin_comb:[BxV]\n",
    "\n",
    "        # Here we assume q_dim == k_dim (dot product attention)\n",
    "\n",
    "        query = query.unsqueeze(1) # [BxQ] -> [Bx1xQ]\n",
    "        keys = keys.transpose(0,1).transpose(1,2) # [TxBxK] -> [BxKxT]\n",
    "        energy = torch.bmm(query, keys) # [Bx1xQ]x[BxKxT] -> [Bx1xT]\n",
    "        energy = F.softmax(energy.mul_(self.scale), dim=2) # scale, normalize\n",
    "\n",
    "        values = values.transpose(0,1) # [TxBxV] -> [BxTxV]\n",
    "        linear_combination = torch.bmm(energy, values).squeeze(1) #[Bx1xT]x[BxTxV] -> [BxV]\n",
    "        return energy, linear_combination\n",
    "\n",
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, pretrained_embedding=None, **config):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['embed_dim'])\n",
    "\n",
    "        # Copy the pretrained embeddings if they exist\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embedding))\n",
    "\n",
    "        self.encoder = Encoder(config['embed_dim'], config['hidden_dim'], config['nlayers'], \n",
    "                               config['dropout'], config['bidirectional'], config['rnn_type'])\n",
    "        attention_dim = config['hidden_dim'] if not config['bidirectional'] else 2 * config['hidden_dim']\n",
    "        self.attention = Attention(attention_dim, attention_dim, attention_dim)\n",
    "        self.decoder = nn.Linear(attention_dim, config['num_classes'])\n",
    "\n",
    "        size = 0\n",
    "        for p in self.parameters():\n",
    "            size += p.nelement()\n",
    "        print('Total parameter size: {}'.format(size))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        outputs, hidden = self.encoder(self.embedding(input))\n",
    "        if isinstance(hidden, tuple): # LSTM\n",
    "            hidden = hidden[1] # take the cell state\n",
    "\n",
    "        if self.encoder.bidirectional: # need to concat the last 2 hidden layers\n",
    "            hidden = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "\n",
    "        energy, linear_combination = self.attention(hidden, outputs, outputs) \n",
    "        logits = self.decoder(linear_combination)\n",
    "        return_dict = {\n",
    "            'pred': logits,\n",
    "            'attention_weights':energy\n",
    "        }\n",
    "\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTorchModel(AbstractSupervisedModel):\n",
    "    def __init__(self, model_class, criterion, optimizer, \n",
    "                 device=torch.device('cpu'), **model_config):\n",
    "        self.model_class = model_class\n",
    "        self.model_config = model_config\n",
    "        self.device = device\n",
    "        self.optimizer_class = optimizer\n",
    "        \n",
    "        self._model = model_class(**model_config).to(self.device)\n",
    "        self.optimizer = optimizer(self.model.parameters(), model_config['lr'])\n",
    "\n",
    "        self.criterion = criterion\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        \"\"\"Call the forward method of the internalized model\n",
    "        \"\"\"\n",
    "        return self.model(X)\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Fit the model on (X, y). \n",
    "        Assumes that the model is in training mode.\n",
    "        \"\"\"\n",
    "        # Train-specific boilerplate code\n",
    "        self.model.train()\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        return_dict = self(X)\n",
    "        logits = return_dict['pred'].view(-1, self.model_config['num_classes'])\n",
    "\n",
    "        loss = self.criterion(logits, y.squeeze())\n",
    "        return_dict['loss'] = loss\n",
    "        \n",
    "        # Optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.model_config['clip'])\n",
    "        self.optimizer.step()\n",
    "        return return_dict\n",
    "\n",
    "    def predict(self, X, return_as_numpy=True, **kwargs):\n",
    "        \"\"\"Return the outputs of the model for inputs X.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return_dict = self(X)\n",
    "\n",
    "            if return_as_numpy:\n",
    "                # Cast everything to numpy\n",
    "                preds = return_dict['pred']\n",
    "                # .cpu() is a no-op if the model is already on cpu\n",
    "                preds = preds.cpu().numpy()\n",
    "                return_dict['pred'] = preds\n",
    "\n",
    "            return return_dict\n",
    "    \n",
    "    def evaluate(self, X, y, **kwargs):\n",
    "        \"\"\"Evaluate the model (compute loss) on (X, y). \n",
    "        Assumes that the model is in evaluation mode.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return_dict = self(X)\n",
    "            logits = return_dict['pred'].view(-1, self.model_config['num_classes'])\n",
    "            loss = self.criterion(logits, y.squeeze())\n",
    "            return_dict['loss'] = loss\n",
    "            return return_dict\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset (re-initialize) the model.\n",
    "        Also resets the internal state of the optimizer.\n",
    "        \"\"\"\n",
    "        self._model = self.model_class(self.model_config).to(self.model_config['device'])\n",
    "        self.optimizer = self.optimizer_class(self.model.parameters(), self.model_config['lr'])\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.model_class = state['model_class']\n",
    "        self.model_config = state['model_config']\n",
    "        self.device = state['device']\n",
    "\n",
    "        # Deserialize model\n",
    "        model = self.model_class(self.config)\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        self._model = model\n",
    "\n",
    "        # Deserialize optimizer\n",
    "        self.optimizer_class = state['optimizer_class']\n",
    "        self.optimizer = self.optimizer_class(self.model.parameters(), self.config.lr)\n",
    "        self.optimizer.load_state_dict(state['optimizer_state'])\n",
    "\n",
    "        # Deserialize loss\n",
    "        loss_class = state['loss_class']\n",
    "        self.criterion = loss_class()\n",
    "        self.criterion.load_state_dict(state['loss_state'])\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = {\n",
    "            'model_class': self.model_class,\n",
    "            'config': self.model_config,\n",
    "            'model_state': self.model.state_dict(),\n",
    "            'optimizer_class': self.optimizer_class,\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'loss_class': self.criterion.__class__,\n",
    "            'loss_state': self.criterion.state_dict(),\n",
    "            'device': self.device\n",
    "        }\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTrainer(AbstractTrainer):\n",
    "    def __init__(self, num_epochs, device, iterator, valid_data=None):\n",
    "        self.epochs = num_epochs\n",
    "        self.valid_data = valid_data\n",
    "        self.device = device\n",
    "        self.iterator = iterator\n",
    "\n",
    "    def train(self,\n",
    "              model,\n",
    "              dataset,\n",
    "              feature_transformer,\n",
    "              label_transform_fun,\n",
    "              **kwargs):\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            total_time = time.time()\n",
    "            for batch_num, (batch_x, batch_y) in enumerate(self.iterator(dataset)):\n",
    "                t = time.time()\n",
    "                X = torch.from_numpy(\n",
    "                    feature_transformer.transform(batch_x).swapaxes(0,1) # swap batch_size and T\n",
    "                    ).to(self.device)\n",
    "                y = torch.from_numpy(\n",
    "                    label_transform_fun(batch_y)\n",
    "                    ).to(self.device)\n",
    "\n",
    "                return_dict = model.fit(X, y)\n",
    "\n",
    "                print(\"[Batch]: {} in {:.5f} seconds, loss={:.5f}\".format(\n",
    "                       batch_num, time.time() - t, return_dict['loss']), \n",
    "                       end='\\r', flush=True)\n",
    "\n",
    "            print(f\"\\nTotal time for train epoch: {time.time() - total_time}\")\n",
    "\n",
    "            total_time = time.time()\n",
    "            for batch_num, (batch_x, batch_y) in enumerate(self.iterator(self.valid_data)):\n",
    "                t = time.time()\n",
    "                X = torch.from_numpy(\n",
    "                    feature_transformer.transform(batch_x).swapaxes(0,1) # swap batch_size and T\n",
    "                    ).to(self.device)\n",
    "                y = torch.from_numpy(\n",
    "                    label_transform_fun(batch_y)\n",
    "                    ).to(self.device)\n",
    "\n",
    "                return_dict = model.evaluate(X, y)\n",
    "                loss = return_dict['loss']\n",
    "                print(\"[Valid]: {} in {:.5f} seconds, loss={:.5f}\".format(\n",
    "                       batch_num, time.time() - t, loss), \n",
    "                       end='\\r', flush=True)\n",
    "\n",
    "            print(f\"\\nTotal time for valid epoch: {time.time() - total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab[finalized: True, size: 2]\n"
     ]
    }
   ],
   "source": [
    "# Model-specific configuration\n",
    "model_config = {\n",
    "    'rnn_type': 'LSTM',\n",
    "    'embed_dim': 300,\n",
    "    'hidden_dim': 150,\n",
    "    'nlayers': 1,\n",
    "    'lr': 1e-3,\n",
    "    'clip': 5,\n",
    "    'epochs': 1,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.,\n",
    "    'bidirectional': True,\n",
    "    'gpu': -1\n",
    "}\n",
    "\n",
    "# Task-specific configuration\n",
    "model_config['vocab_size'] = len(vocab)\n",
    "label_vocab = fields['label'].vocab\n",
    "model_config['num_classes'] = len(label_vocab)\n",
    "model_config['pretrained_embedding'] = embeddings\n",
    "\n",
    "device = torch.device('cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter size: 3543002\n",
      "[Batch]: 781 in 0.34659 seconds, loss=0.53425\n",
      "Total time for train epoch: 807.2142312526703\n",
      "[Valid]: 781 in 0.03590 seconds, loss=0.71830\n",
      "Total time for valid epoch: 79.69959831237793\n"
     ]
    }
   ],
   "source": [
    "data_iterator = Iterator(batch_size=32)\n",
    "\n",
    "trainer = TorchTrainer(model_config['epochs'], device, data_iterator, imdb_test)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "experiment = Experiment(MyTorchModel, trainer=trainer)\n",
    "model = experiment.fit(\n",
    "    imdb_train,\n",
    "    model_kwargs={\n",
    "        'model_class': AttentionRNN, \n",
    "        'criterion': criterion,\n",
    "        'optimizer': torch.optim.Adam,\n",
    "        'device': device,\n",
    "        **model_config\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.MyTorchModel'>: it's not the same object as __main__.MyTorchModel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f442d36c4530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_save_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdump_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitted_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.MyTorchModel'>: it's not the same object as __main__.MyTorchModel"
     ]
    }
   ],
   "source": [
    "# Check serialization for _model_ only (should be for experiment as well)\n",
    "import pickle\n",
    "fitted_model = experiment.model\n",
    "\n",
    "model_save_file = 'model.pt'\n",
    "with open(model_save_file, 'wb') as dump_file:\n",
    "    pickle.dump(fitted_model, dump_file)\n",
    "\n",
    "with open(model_save_file, 'rb') as load_file:\n",
    "    loaded_model = pickle.load(load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For instance: ['This movie is horrible'], the prediction is: 0, with logits: [-5.358241   5.3401985]\n",
      "For instance: ['This movie is great!'], the prediction is: 1, with logits: [ 3.6698027 -3.4615505]\n"
     ]
    }
   ],
   "source": [
    "ft = experiment.feature_transformer\n",
    "cast_to_torch_transformer = lambda t: torch.from_numpy(ft.transform(t).swapaxes(0,1)).to(device)\n",
    "\n",
    "pipe = Pipeline(\n",
    "  fields = list(fields.values()),\n",
    "  example_format = 'list',\n",
    "  feature_transformer = cast_to_torch_transformer,\n",
    "  model = fitted_model\n",
    "  )\n",
    "\n",
    "instances = [\n",
    "        ['This movie is horrible'], \n",
    "        ['This movie is great!']\n",
    "]\n",
    "\n",
    "# Make IMDB labels \"positive\" and \"negative\"\n",
    "for instance in instances:\n",
    "    prediction = pipe.predict_raw(instance)\n",
    "    print(f\"For instance: {instance}, the prediction is: {fields['label'].vocab.itos[prediction.argmax()]}, with logits: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
