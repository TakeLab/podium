{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import takepod\n",
    "from takepod.datasets import BucketIterator, Iterator, BasicSupervisedImdbDataset\n",
    "from takepod.storage import Field, Vocab\n",
    "from takepod.storage.vectorizers.impl import GloVe\n",
    "from takepod.models import Experiment, AbstractSupervisedModel\n",
    "from takepod.models.trainer import AbstractTrainer\n",
    "\n",
    "def lowercase(raw, data):\n",
    "    return raw, [d.lower() for d in data]\n",
    "\n",
    "def max_length(raw, data, length=200):\n",
    "    return raw, data[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = BasicSupervisedImdbDataset.get_default_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fields():\n",
    "    # Define the vocabulary\n",
    "    vocab = Vocab(max_size=10000, min_freq=5)\n",
    "    text = Field(name='text', vocab=vocab, tokenizer='spacy', store_as_raw=False)\n",
    "    # Add preprpocessing hooks to model\n",
    "    # 1. Lowercase\n",
    "    text.add_posttokenize_hook(lowercase)\n",
    "    text.add_posttokenize_hook(max_length)\n",
    "    # Improve readability: LabelField\n",
    "    label = Field(name='label', vocab=Vocab(specials=()), is_target=True, tokenize=False)\n",
    "    return {text.name : text, label.name: label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = create_fields()\n",
    "imdb_train, imdb_test = BasicSupervisedImdbDataset.get_train_test_dataset(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct vectoziter based on vocab\n",
    "vocab = fields['text'].vocab\n",
    "vectorizer = GloVe()\n",
    "vectorizer.load_vocab(vocab)\n",
    "embeddings = vectorizer.get_embedding_matrix(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.04656    0.21318   -0.0074364 ...  0.0090611 -0.20989    0.053913 ]\n",
      " ...\n",
      " [-0.24734    0.019346   0.13974   ...  0.34035    0.0824     0.38554  ]\n",
      " [ 0.67287   -0.43249    0.1106    ... -0.16644    0.21169    0.45995  ]\n",
      " [ 0.034368   0.22004    0.14626   ... -0.18641   -0.032439   0.24544  ]]\n"
     ]
    }
   ],
   "source": [
    "# Works on simplify vectorizer branch\n",
    "embeddings = GloVe().load_vocab(vocab)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = Iterator(dataset=imdb_train, batch_size=32)\n",
    "valid_iterator = Iterator(dataset=imdb_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "RNNS = ['LSTM', 'GRU']\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, nlayers=1, dropout=0.,\n",
    "                   bidirectional=True, rnn_type='GRU'):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        assert rnn_type in RNNS, 'Use one of the following: {}'.format(str(RNNS))\n",
    "        rnn_cell = getattr(nn, rnn_type) # fetch constructor from torch.nn, cleaner than if\n",
    "        self.rnn = rnn_cell(embedding_dim, hidden_dim, nlayers, \n",
    "                                dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.rnn(input, hidden)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scale = 1. / math.sqrt(query_dim)\n",
    "\n",
    "    def forward(self, query, keys, values):\n",
    "        # Query = [BxQ]\n",
    "        # Keys = [TxBxK]\n",
    "        # Values = [TxBxV]\n",
    "        # Outputs = a:[TxB], lin_comb:[BxV]\n",
    "\n",
    "        # Here we assume q_dim == k_dim (dot product attention)\n",
    "\n",
    "        query = query.unsqueeze(1) # [BxQ] -> [Bx1xQ]\n",
    "        keys = keys.transpose(0,1).transpose(1,2) # [TxBxK] -> [BxKxT]\n",
    "        energy = torch.bmm(query, keys) # [Bx1xQ]x[BxKxT] -> [Bx1xT]\n",
    "        energy = F.softmax(energy.mul_(self.scale), dim=2) # scale, normalize\n",
    "\n",
    "        values = values.transpose(0,1) # [TxBxV] -> [BxTxV]\n",
    "        linear_combination = torch.bmm(energy, values).squeeze(1) #[Bx1xT]x[BxTxV] -> [BxV]\n",
    "        return energy, linear_combination\n",
    "\n",
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.config = cfg\n",
    "        self.embedding = nn.Embedding(cfg.vocab_size, cfg.embed_dim)\n",
    "        self.encoder = Encoder(cfg.embed_dim, cfg.hidden_dim, cfg.nlayers, \n",
    "                               cfg.dropout, cfg.bidirectional, cfg.rnn_type)\n",
    "        attention_dim = cfg.hidden_dim if not cfg.bidirectional else 2 * cfg.hidden_dim\n",
    "        self.attention = Attention(attention_dim, attention_dim, attention_dim)\n",
    "        self.decoder = nn.Linear(attention_dim, cfg.num_classes)\n",
    "\n",
    "        size = 0\n",
    "        for p in self.parameters():\n",
    "            size += p.nelement()\n",
    "        print('Total param size: {}'.format(size))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        outputs, hidden = self.encoder(self.embedding(input))\n",
    "        if isinstance(hidden, tuple): # LSTM\n",
    "            hidden = hidden[1] # take the cell state\n",
    "\n",
    "        if self.encoder.bidirectional: # need to concat the last 2 hidden layers\n",
    "            hidden = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "\n",
    "        energy, linear_combination = self.attention(hidden, outputs, outputs) \n",
    "        logits = self.decoder(linear_combination)\n",
    "        return_dict = {\n",
    "            'pred': logits,\n",
    "            'attention_weights':energy\n",
    "        }\n",
    "\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTorchModel(AbstractSupervisedModel):\n",
    "    def __init__(self, model_class, config, criterion, optimizer):\n",
    "        self.model_class = model_class\n",
    "        self.config = config\n",
    "        self._model = model_class(config)\n",
    "        self.optimizer = optimizer(self.model.parameters(), config.lr)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        # This is a _step_ in the iteration process.\n",
    "        # Should assume model is in training mode\n",
    "        \n",
    "        # Train-specific code\n",
    "        self.model.train()\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        return_dict = self(X)\n",
    "        logits = return_dict['pred']\n",
    "        #print(logits.view(-1, self.config.num_classes), y.squeeze())\n",
    "        loss = self.criterion(logits.view(-1, self.config.num_classes), y.squeeze())\n",
    "        return_dict['loss'] = loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip)\n",
    "        self.optimizer.step()\n",
    "        return return_dict\n",
    "        \n",
    "    def predict(self, X, **kwargs):\n",
    "        # Assumes that the model is in _eval_ mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return_dict = self(X)\n",
    "            return return_dict\n",
    "    \n",
    "    def evaluate(self, X, y, **kwargs):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return_dict = self(X)\n",
    "            logits = return_dict['pred']\n",
    "            loss = self.criterion(logits.view(-1, self.config.num_classes), y.squeeze())\n",
    "            return_dict['loss'] = loss\n",
    "            return return_dict\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        # Restart model\n",
    "        self._model = self.model_class(self.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTrainer(AbstractTrainer):\n",
    "    def __init__(self, num_epochs, valid_iterator=None):\n",
    "        self.epochs = num_epochs\n",
    "        self.valid_iterator = valid_iterator\n",
    "    \n",
    "    def train(self,\n",
    "              model: AbstractSupervisedModel,\n",
    "              iterator: Iterator,\n",
    "              feature_transformer,\n",
    "              label_transform_fun,\n",
    "              **kwargs):\n",
    "        # Actual training loop\n",
    "        # Single training epoch\n",
    "        for batch_num, (batch_x, batch_y) in enumerate(iterator):\n",
    "            t = time.time()\n",
    "            X = torch.from_numpy(\n",
    "                feature_transformer.transform(batch_x).swapaxes(0,1) # swap batch_size and T\n",
    "                )\n",
    "            y = torch.from_numpy(\n",
    "                label_transform_fun(batch_y)\n",
    "                )\n",
    "            return_dict = model.fit(X, y)\n",
    "            \n",
    "            print(\"[Batch]: {}/{} in {:.5f} seconds, loss={:.5f}\".format(\n",
    "                   batch_num, len(iterator), time.time() - t, return_dict['loss']), \n",
    "                   end='\\r', flush=True)\n",
    "            \n",
    "        \n",
    "        for batch_num, batch_x, batch_y in enumerate(self.valid_iterator):\n",
    "            X = feature_transformer.transform(batch_x)\n",
    "            y = label_transform_fun(batch_y)\n",
    "\n",
    "            return_dict = model.evaluate(X, y)\n",
    "            loss = return_dict['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, *args, **kwargs): \n",
    "        dict.__init__(self, *args, **kwargs)     \n",
    "            \n",
    "    def __getattr__(self, key):\n",
    "        #print(key)\n",
    "        return self[key]\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        #print(key, value)\n",
    "        self[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "label_vocab = fields['label'].vocab\n",
    "# Ugly but just to check\n",
    "config_dict = {\n",
    "    'rnn_type': 'LSTM',\n",
    "    'embed_dim': 300,\n",
    "    'hidden_dim': 300,\n",
    "    'nlayers': 1,\n",
    "    'lr': 1e-3,\n",
    "    'clip': 5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.,\n",
    "    'bidirectional': True,\n",
    "    'cuda': False,\n",
    "    'vocab_size': len(vocab),\n",
    "    'num_classes': len(label_vocab)\n",
    "    \n",
    "}\n",
    "\n",
    "config = Config(config_dict)\n",
    "#model = TorchModel(AttentionRNN, config, criterion, optimizer=torch.optim.Adam)\n",
    "\n",
    "#optimizer = optimizer = torch.optim.Adam(model.parameters(), config.lr, amsgrad=True)\n",
    "trainer = TorchTrainer(config.epochs, valid_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total param size: 4446002\n",
      "[Batch]: 39/782 in 0.75744 seconds, loss=0.63336\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-adb944af4305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m'config'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m'criterion'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     },\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/takepod-0.0.1-py3.7.egg/takepod/models/experiment.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, model_kwargs, trainer_kwargs, feature_transformer, trainer, training_iterator_callable)\u001b[0m\n\u001b[1;32m    196\u001b[0m                          \u001b[0mtrainer_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                          \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                          training_iterator_callable)\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     def partial_fit(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/takepod-0.0.1-py3.7.egg/takepod/models/experiment.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, dataset, trainer_kwargs, trainer, training_iterator_callable)\u001b[0m\n\u001b[1;32m    251\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_transform_fun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                       **trainer_args)\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     def predict(self,\n",
      "\u001b[0;32m<ipython-input-10-86df4a500248>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, iterator, feature_transformer, label_transform_fun, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mlabel_transform_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 )\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             print(\"[Batch]: {}/{} in {:.5f} seconds, loss={:.5f}\".format(\n",
      "\u001b[0;32m<ipython-input-9-f70b2f085538>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mreturn_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "train_iterator = partial(Iterator, batch_size=32, shuffle=True)\n",
    "valid_iterator = Iterator(dataset=imdb_train, batch_size=32, shuffle=True)\n",
    "\n",
    "experiment = Experiment(MyTorchModel, trainer=trainer, \n",
    "                        training_iterator_callable=train_iterator)\n",
    "experiment.fit(\n",
    "    imdb_train,\n",
    "    model_kwargs={\n",
    "        'model_class': AttentionRNN, \n",
    "        'config': config, \n",
    "        'criterion': criterion,\n",
    "        'optimizer': torch.optim.Adam\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
