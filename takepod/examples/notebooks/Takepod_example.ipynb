{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import takepod\n",
    "from takepod.datasets import BucketIterator, Iterator, BasicSupervisedImdbDataset\n",
    "from takepod.storage import Field, Vocab\n",
    "from takepod.storage.vectorizers.impl import GloVe\n",
    "from takepod.models import Experiment, AbstractSupervisedModel\n",
    "from takepod.models.trainer import AbstractTrainer\n",
    "\n",
    "def lowercase(raw, data):\n",
    "    return raw, [d.lower() for d in data]\n",
    "\n",
    "def max_length(raw, data, length=200):\n",
    "    return raw, data[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = BasicSupervisedImdbDataset.get_default_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fields():\n",
    "    # Define the vocabulary\n",
    "    vocab = Vocab(max_size=10000, min_freq=5)\n",
    "    text = Field(name='text', vocab=vocab, tokenizer='spacy', store_as_raw=False)\n",
    "    # Add preprpocessing hooks to model\n",
    "    # 1. Lowercase\n",
    "    text.add_posttokenize_hook(lowercase)\n",
    "    text.add_posttokenize_hook(max_length)\n",
    "    # Improve readability: LabelField\n",
    "    label = Field(name='label', vocab=Vocab(specials=()), is_target=True, tokenize=False)\n",
    "    return {text.name : text, label.name: label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = create_fields()\n",
    "imdb_train, imdb_test = BasicSupervisedImdbDataset.get_train_test_dataset(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct vectoziter based on vocab\n",
    "vocab = fields['text'].vocab\n",
    "vectorizer = GloVe()\n",
    "vectorizer.load_vocab(vocab)\n",
    "embeddings = vectorizer.get_embedding_matrix(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.04656    0.21318   -0.0074364 ...  0.0090611 -0.20989    0.053913 ]\n",
      " ...\n",
      " [-0.24734    0.019346   0.13974   ...  0.34035    0.0824     0.38554  ]\n",
      " [ 0.67287   -0.43249    0.1106    ... -0.16644    0.21169    0.45995  ]\n",
      " [ 0.034368   0.22004    0.14626   ... -0.18641   -0.032439   0.24544  ]]\n"
     ]
    }
   ],
   "source": [
    "# Works on simplify vectorizer branch\n",
    "embeddings = GloVe().load_vocab(vocab)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = Iterator(dataset=imdb_train, batch_size=32)\n",
    "valid_iterator = Iterator(dataset=imdb_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "RNNS = ['LSTM', 'GRU']\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, nlayers=1, dropout=0.,\n",
    "                   bidirectional=True, rnn_type='GRU'):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        assert rnn_type in RNNS, 'Use one of the following: {}'.format(str(RNNS))\n",
    "        rnn_cell = getattr(nn, rnn_type) # fetch constructor from torch.nn, cleaner than if\n",
    "        self.rnn = rnn_cell(embedding_dim, hidden_dim, nlayers, \n",
    "                                dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.rnn(input, hidden)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scale = 1. / math.sqrt(query_dim)\n",
    "\n",
    "    def forward(self, query, keys, values):\n",
    "        # Query = [BxQ]\n",
    "        # Keys = [TxBxK]\n",
    "        # Values = [TxBxV]\n",
    "        # Outputs = a:[TxB], lin_comb:[BxV]\n",
    "\n",
    "        # Here we assume q_dim == k_dim (dot product attention)\n",
    "\n",
    "        query = query.unsqueeze(1) # [BxQ] -> [Bx1xQ]\n",
    "        keys = keys.transpose(0,1).transpose(1,2) # [TxBxK] -> [BxKxT]\n",
    "        energy = torch.bmm(query, keys) # [Bx1xQ]x[BxKxT] -> [Bx1xT]\n",
    "        energy = F.softmax(energy.mul_(self.scale), dim=2) # scale, normalize\n",
    "\n",
    "        values = values.transpose(0,1) # [TxBxV] -> [BxTxV]\n",
    "        linear_combination = torch.bmm(energy, values).squeeze(1) #[Bx1xT]x[BxTxV] -> [BxV]\n",
    "        return energy, linear_combination\n",
    "\n",
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.config = cfg\n",
    "        self.embedding = nn.Embedding(cfg.vocab_size, cfg.embed_dim)\n",
    "        self.encoder = Encoder(cfg.embed_dim, cfg.hidden_dim, cfg.nlayers, \n",
    "                               cfg.dropout, cfg.bidirectional, cfg.rnn_type)\n",
    "        attention_dim = cfg.hidden_dim if not cfg.bidirectional else 2 * cfg.hidden_dim\n",
    "        self.attention = Attention(attention_dim, attention_dim, attention_dim)\n",
    "        self.decoder = nn.Linear(attention_dim, cfg.num_classes)\n",
    "\n",
    "        size = 0\n",
    "        for p in self.parameters():\n",
    "            size += p.nelement()\n",
    "        print('Total param size: {}'.format(size))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        outputs, hidden = self.encoder(self.embedding(input))\n",
    "        if isinstance(hidden, tuple): # LSTM\n",
    "            hidden = hidden[1] # take the cell state\n",
    "\n",
    "        if self.encoder.bidirectional: # need to concat the last 2 hidden layers\n",
    "            hidden = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "\n",
    "        energy, linear_combination = self.attention(hidden, outputs, outputs) \n",
    "        logits = self.decoder(linear_combination)\n",
    "        return_dict = {\n",
    "            'pred': logits,\n",
    "            'attention_weights':energy\n",
    "        }\n",
    "\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTorchModel(AbstractSupervisedModel):\n",
    "    def __init__(self, model_class, config, criterion, optimizer):\n",
    "        self.model_class = model_class\n",
    "        self.config = config\n",
    "        self._model = model_class(config)\n",
    "        self.optimizer = optimizer(self.model.parameters(), config.lr)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        # This is a _step_ in the iteration process.\n",
    "        # Should assume model is in training mode\n",
    "        \n",
    "        # Train-specific code\n",
    "        self.model.train()\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        return_dict = self(X)\n",
    "        logits = return_dict['pred']\n",
    "        #print(logits.view(-1, self.config.num_classes), y.squeeze())\n",
    "        loss = self.criterion(logits.view(-1, self.config.num_classes), y.squeeze())\n",
    "        return_dict['loss'] = loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip)\n",
    "        self.optimizer.step()\n",
    "        return return_dict\n",
    "        \n",
    "    def predict(self, X, **kwargs):\n",
    "        # Assumes that the model is in _eval_ mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return_dict = self(X)\n",
    "            return return_dict\n",
    "    \n",
    "    def evaluate(self, X, y, **kwargs):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return_dict = self(X)\n",
    "            logits = return_dict['pred']\n",
    "            loss = self.criterion(logits.view(-1, self.config.num_classes), y.squeeze())\n",
    "            return_dict['loss'] = loss\n",
    "            return return_dict\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        # Restart model\n",
    "        self._model = self.model_class(self.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTrainer(AbstractTrainer):\n",
    "    def __init__(self, num_epochs, valid_iterator=None):\n",
    "        self.epochs = num_epochs\n",
    "        self.valid_iterator = valid_iterator\n",
    "    \n",
    "    def train(self,\n",
    "              model: AbstractSupervisedModel,\n",
    "              iterator: Iterator,\n",
    "              feature_transformer,\n",
    "              label_transform_fun,\n",
    "              **kwargs):\n",
    "        # Actual training loop\n",
    "        # Single training epoch\n",
    "        for batch_num, (batch_x, batch_y) in enumerate(iterator):\n",
    "            t = time.time()\n",
    "            X = torch.from_numpy(\n",
    "                feature_transformer.transform(batch_x).swapaxes(0,1) # swap batch_size and T\n",
    "                )\n",
    "            y = torch.from_numpy(\n",
    "                label_transform_fun(batch_y)\n",
    "                )\n",
    "            return_dict = model.fit(X, y)\n",
    "            \n",
    "            print(\"[Batch]: {}/{} in {:.5f} seconds, loss={:.5f}\".format(\n",
    "                   batch_num, len(iterator), time.time() - t, return_dict['loss']), \n",
    "                   end='\\r', flush=True)\n",
    "            \n",
    "        \n",
    "        for batch_num, batch_x, batch_y in enumerate(self.valid_iterator):\n",
    "            X = feature_transformer.transform(batch_x)\n",
    "            y = label_transform_fun(batch_y)\n",
    "\n",
    "            return_dict = model.evaluate(X, y)\n",
    "            loss = return_dict['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, *args, **kwargs): \n",
    "        dict.__init__(self, *args, **kwargs)     \n",
    "            \n",
    "    def __getattr__(self, key):\n",
    "        #print(key)\n",
    "        return self[key]\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        #print(key, value)\n",
    "        self[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "label_vocab = fields['label'].vocab\n",
    "# Ugly but just to check\n",
    "config_dict = {\n",
    "    'rnn_type': 'LSTM',\n",
    "    'embed_dim': 300,\n",
    "    'hidden_dim': 300,\n",
    "    'nlayers': 1,\n",
    "    'lr': 1e-3,\n",
    "    'clip': 5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.,\n",
    "    'bidirectional': True,\n",
    "    'cuda': False,\n",
    "    'vocab_size': len(vocab),\n",
    "    'num_classes': len(label_vocab)\n",
    "    \n",
    "}\n",
    "\n",
    "config = Config(config_dict)\n",
    "#model = TorchModel(AttentionRNN, config, criterion, optimizer=torch.optim.Adam)\n",
    "\n",
    "#optimizer = optimizer = torch.optim.Adam(model.parameters(), config.lr, amsgrad=True)\n",
    "trainer = TorchTrainer(config.epochs, valid_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For BucketIterator to work, either sort_key or bucket_sort_key must be != None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For BucketIterator to work, either sort_key or bucket_sort_key must be != None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-50bf08f7e4f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucketIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvalid_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimdb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m experiment = Experiment(TorchModel, trainer=trainer, \n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/takepod-0.0.1-py3.7.egg/takepod/datasets/iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, sort_key, shuffle, seed, look_ahead_multiplier, bucket_sort_key)\u001b[0m\n\u001b[1;32m    420\u001b[0m                         \u001b[0;34m\"bucket_sort_key must be != None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         super().__init__(dataset,\n",
      "\u001b[0;31mValueError\u001b[0m: For BucketIterator to work, either sort_key or bucket_sort_key must be != None."
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "train_iterator = partial(BucketIterator, batch_size=32, shuffle=True, bucket_sort_key=lambda x: len(x[0]))\n",
    "valid_iterator = BucketIterator(dataset=imdb_train, batch_size=32, shuffle=True)\n",
    "\n",
    "experiment = Experiment(TorchModel, trainer=trainer, \n",
    "                        training_iterator_callable=train_iterator)\n",
    "experiment.fit(\n",
    "    imdb_train,\n",
    "    model_kwargs={\n",
    "        'model_class': AttentionRNN, \n",
    "        'config': config, \n",
    "        'criterion': criterion,\n",
    "        'optimizer': torch.optim.Adam\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
