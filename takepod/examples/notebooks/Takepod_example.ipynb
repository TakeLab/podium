{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import takepod\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from takepod.datasets import BasicSupervisedImdbDataset\n",
    "from takepod.storage import LabelField, Field, Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading & preprocessing\n",
    "\n",
    "When using `podium` you have three options for data loading:\n",
    "1. Use one of our built-in datasets\n",
    "2. Use a flexible data loader from a predefined format (`TabularDataset` loads from `.csv`, `.tsv` files)\n",
    "3. Write your own data loader for a dataset in a custom format\n",
    "\n",
    "### IMDB sentiment classification\n",
    "\n",
    "![Imdb logo](img/imdb_logo_small.png)\n",
    "\n",
    "For this walkthough, we will use the [IMDB sentiment classification dataset](https://ai.stanford.edu/~amaas/data/sentiment/). This dataset is built-in, so let's check what exactly does that mean.\n",
    "\n",
    "- Each built-in dataset has a static method `get_dataset_splits` which downloads and caches the splits for that model and returns them as a tuple (train, valid?, test).\n",
    "  - Note: the IMDB dataset only has a train and test split\n",
    "- We will first load the IMDB dataset with default `Fields` (preprocessing pipelines) and check whether we might want to modify something.\n",
    "- You can inspect the default fields by calling the `get_default_fields` static method of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train, imdb_test = BasicSupervisedImdbDataset.get_dataset_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in a single dataset instance:\n",
      "==================================================\n",
      "(None, ['I', 'am', 'and', 'was', 'very', 'entertained', 'by', 'the', 'movie', '.', 'It', 'was', 'my', 'all', 'time', 'favorite', 'movie', 'of', '1976', '.', 'Being', 'raised', 'in', 'the', '70', \"'s\", ',', 'I', 'was', 'so', 'in', 'love', 'with', 'Kris', 'Kristoffersons', 'look', 'and', 'demeanor', ',', 'of', 'course', 'I', 'am', 'no', 'movie', 'critic', ',', 'but', 'for', 'the', 'time', 'era', ',', 'I', 'think', 'it', 'was', 'very', 'good', '.', 'I', 'very', 'much', 'like', 'the', 'combo', 'of', 'Streisand', 'and', 'Kristofferson', '.', 'I', 'thought', 'they', 'worked', 'very', 'well', 'together', '.', 'I', 'have', 'seen', 'the', 'movie', 'many', 'times', 'and', 'still', 'love', 'the', 'two', 'of', 'them', 'as', 'Esther', 'and', 'John', 'Norman', '.', 'I', 'am', 'a', 'very', 'huge', 'fan', 'of', 'Kris', 'and', 'see', 'him', 'in', 'concert', 'when', 'I', 'can', '.', 'What', 'a', 'talented', 'singer', 'song', 'writer', ',', 'not', 'to', 'mention', ',', 'actor', '.', 'I', 'have', 'seen', 'him', 'in', 'many', 'movies', ',', 'but', 'still', 'think', 'back', 'to', 'A', 'star', 'is', 'Born', '.'])\n",
      "('positive', None)\n",
      "==================================================\n",
      "Input text length interval [11, 2789] \n",
      "Average length 272.42864 +- 202.5628456251304\n"
     ]
    }
   ],
   "source": [
    "first_instance = imdb_train[0]\n",
    "text, label = first_instance.text, first_instance.label\n",
    "\n",
    "\n",
    "# Note that the text is cased\n",
    "print(\"Data in a single dataset instance:\")\n",
    "print(\"=\"*50)\n",
    "print(text)\n",
    "print(label)\n",
    "print(\"=\"*50)\n",
    "\n",
    "def get_text_statistics(dataset):\n",
    "    instance_lengths = [len(ex.text[1]) for ex in dataset]\n",
    "    print(f\"Input text length interval [{min(instance_lengths)}, {max(instance_lengths)}] \\n\" \n",
    "                             f\"Average length {np.mean(instance_lengths)} +- {np.std(instance_lengths)}\")\n",
    "get_text_statistics(imdb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using hooks during data preprocessing\n",
    "The average length of instances in the dataset is large, while the longest instance has 2789 tokens. \n",
    "Instances of this length are likely to cause memory issues when batched and transferred to GPU, so we would like to limit this. We might also want only lowercase data in our instances.\n",
    "\n",
    "TODO: add data processing graph (cf prez)\n",
    "\n",
    "We can implement this ourselves easily by adding `hooks` to our model. Hooks are methods with a standardized signature which view and modify the data flowing through the preprocessing pipeline at **two** points.\n",
    "1. **Pre-tokenization hooks**:\n",
    "  - pre-tokenization hooks work on raw data (the loaded input string). You might want to lowercase data during pre-tokenization, but keep in mind that most tokenizers (such as `spacy`) are sensitive to casing and might produce bad results. Since we use `spacy` as the `IMDB` tokenizer, this is not a good choice and we might want to delegate lowercasing to post-tokenization.\n",
    "2. **Post-tokenization hooks**:\n",
    "  - post-tokenization hooks work on raw **and** tokenized data. Here you might want to limit the length of your instances to a fixed amount or filter out stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The signature of post-tokenization hooks has *two* arguments: raw and tokenized data\n",
    "def lowercase(raw, tokenized):\n",
    "    \"\"\"Applies lowercasing as a post-tokenization hook\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Raw : str\n",
    "        the untokenized input data\n",
    "    Tokenized: list(str)\n",
    "        list of tokens.\n",
    "    Returns\n",
    "    -------\n",
    "    Raw: str \n",
    "        unmodified input\n",
    "    Tokenized: list(str) \n",
    "        lowercased tokenized data\n",
    "    \"\"\"\n",
    "    return raw, [token.lower() for token in tokenized]\n",
    "\n",
    "def max_length(raw, data, length=200):\n",
    "    \"\"\"Applies lowercasing as a post-tokenization hook\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Raw : str\n",
    "        the untokenized input data\n",
    "    Tokenized: list(str)\n",
    "        list of tokens.\n",
    "    Length: int\n",
    "        maximum length for each instance \n",
    "    Returns\n",
    "    -------\n",
    "    Raw: str \n",
    "        unmodified input\n",
    "    Tokenized: list(str) \n",
    "        tokenized data truncated to `length`\n",
    "    \"\"\"\n",
    "    return raw, data[:length]\n",
    "\n",
    "\n",
    "def create_fields():\n",
    "    # Define the vocabulary\n",
    "    max_vocab_size = 10000\n",
    "    min_frequency = 5\n",
    "    vocab = Vocab(max_size=max_vocab_size, min_freq=min_frequency)\n",
    "\n",
    "    text = Field(name='text', vocab=vocab, tokenizer='spacy', store_as_raw=False)\n",
    "    # Add preprpocessing hooks to model\n",
    "    # 1. Lowercase\n",
    "    text.add_posttokenize_hook(lowercase)\n",
    "    # 2. Truncate to length\n",
    "    text.add_posttokenize_hook(max_length)\n",
    "\n",
    "    label = LabelField(name='label', vocab = Vocab(specials=()))\n",
    "    return {text.name : text, label.name: label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our modified fields and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = create_fields()\n",
    "# TODO: remove Dataset, Basic, Supervised from IMDB name\n",
    "imdb_train, imdb_test = BasicSupervisedImdbDataset.get_dataset_splits(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text length interval [11, 200] \n",
      "Average length 170.3254 +- 41.14426515129417\n"
     ]
    }
   ],
   "source": [
    "# Check whether the preprocessing worked\n",
    "get_text_statistics(imdb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained embeddings\n",
    "\n",
    "In most use-cases, we want to use pre-trained word embeddings. With `podium`, this process is incredibly simple. If your field uses a vocabulary, it has already built an inventory of tokens for your dataset.\n",
    "\n",
    "`Podium` offers a number of implemented `vectorizers` and a class ([BasicVectorStorage](https://github.com/mttk/takepod/blob/master/takepod/storage/vectorizers/vectorizer.py#L218)) which is able to load the standardized word2vec-style format of word embeddings from disk.\n",
    "\n",
    "For example, we will use the [GloVe](https://nlp.stanford.edu/projects/glove/) vectors. The procedure to load these vectors has two steps:\n",
    "1. Initialize the vector class, which sets all the required paths\n",
    "  - Right now, the vectors are not yet loaded from disk as you usually don't want to load the full file\n",
    "2. Get the vectors for a pre-defined list of words by calling `load_vocab`\n",
    "  - The argument can be a `Vocab` object (which is itself an `iterable` of strings), or any sequence of strings\n",
    "  \n",
    "The output of the function call is a numpy matrix of word embeddings which you can then pass to your model to initialize the embedding matrix or to be used otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocabulary of size: 10000 loaded embedding matrix of shape: (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "from takepod.storage.vectorizers.impl import GloVe\n",
    "vocab = fields['text'].vocab\n",
    "embeddings = GloVe().load_vocab(vocab)\n",
    "print(f\"For vocabulary of size: {len(vocab)} loaded embedding matrix of shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define & train a model\n",
    "\n",
    "Now we need to train a concrete model on our data!\n",
    "\n",
    "We will use a pre-defined RNN classifier with self-attention as our model.\n",
    "The model, which is implemented in pytorch, needs to be wrapped in the `podium.model` interface so other convenience classes can be used. In this case, the classes you need to use are:\n",
    "\n",
    "- `podium.model` subclass: \n",
    "  - Exposes abstract methods required to train or evaluate the model, or predict on raw data\n",
    "- `podium.trainer` subclass:\n",
    "  - Handles the data <-> model communication (e.g. batching, early stopping). The user only implements this class but does not explicitly use its methods.\n",
    "- `podium.experiment` instance:\n",
    "  - Wraps the model and its parameters to simplify multiple restarts with different choices of hyperparameters (in order to use grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rnn_type': 'LSTM', 'embed_dim': 300, 'hidden_dim': 150, 'nlayers': 1, 'lr': 0.001, 'clip': 5, 'epochs': 1, 'batch_size': 32, 'dropout': 0.0, 'bidirectional': True, 'gpu': -1, 'num_classes': 2, 'vocab_size': 10000, 'pretrained_embedding': array([[ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
      "         0.       ],\n",
      "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
      "         0.       ],\n",
      "       [ 0.04656  ,  0.21318  , -0.0074364, ...,  0.0090611, -0.20989  ,\n",
      "         0.053913 ],\n",
      "       ...,\n",
      "       [-0.24734  ,  0.019346 ,  0.13974  , ...,  0.34035  ,  0.0824   ,\n",
      "         0.38554  ],\n",
      "       [ 0.67287  , -0.43249  ,  0.1106   , ..., -0.16644  ,  0.21169  ,\n",
      "         0.45995  ],\n",
      "       [ 0.034368 ,  0.22004  ,  0.14626  , ..., -0.18641  , -0.032439 ,\n",
      "         0.24544  ]])}\n"
     ]
    }
   ],
   "source": [
    "# First, we will define the hyperparameters for our model. \n",
    "# These are only used when a concrete model is trained, and can be changed between calls.\n",
    "model_config = {\n",
    "    'rnn_type': 'LSTM',\n",
    "    'embed_dim': 300,\n",
    "    'hidden_dim': 150,\n",
    "    'nlayers': 1,\n",
    "    'lr': 1e-3,\n",
    "    'clip': 5,\n",
    "    'epochs': 1,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.,\n",
    "    'bidirectional': True,\n",
    "    'gpu': -1\n",
    "}\n",
    "\n",
    "# Task-specific metadata\n",
    "label_vocab = fields['label'].vocab\n",
    "model_config['num_classes'] = len(label_vocab)\n",
    "model_config['vocab_size'] = len(vocab)\n",
    "model_config['pretrained_embedding'] = embeddings\n",
    "# Run on CPU since we don't have a GPU on this machine\n",
    "device = torch.device('cpu:0')\n",
    "# Define the model criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter size: 3543002\n",
      "[Batch]: 781 in 0.32565 seconds, loss=0.55139\n",
      "Total time for train epoch: 397.8200333118439\n",
      "[Valid]: 781 in 0.03639 seconds, loss=0.67276\n",
      "Total time for valid epoch: 76.19551157951355\n"
     ]
    }
   ],
   "source": [
    "from takepod.datasets import Iterator\n",
    "from takepod.models import Experiment\n",
    "\n",
    "from takepod.models.impl.pytorch import TorchTrainer, TorchModel, AttentionRNN\n",
    "\n",
    "data_iterator = Iterator(batch_size=32)\n",
    "\n",
    "trainer = TorchTrainer(model_config['epochs'], device, data_iterator, imdb_test)\n",
    "experiment = Experiment(TorchModel, trainer=trainer)\n",
    "\n",
    "model = experiment.fit(\n",
    "    imdb_train, # Data on which to fit the model\n",
    "    model_kwargs={ # Arguments passed to the model constructor\n",
    "        'model_class': AttentionRNN, # The wrapped concrete model\n",
    "        'criterion': criterion, # The loss for the concrete model\n",
    "        'optimizer': torch.optim.Adam, # Optimizer _class_\n",
    "        'device': device, # The device to store the data on\n",
    "        **model_config # Delegated to the concrete model\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f442d36c4530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/takepod-0.0.1-py3.7.egg/takepod/models/impl/pytorch/models.py\u001b[0m in \u001b[0;36m__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Deserialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/takepod-0.0.1-py3.7.egg/takepod/models/impl/pytorch/sequence_classification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pretrained_embedding, **config)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttentionRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embed_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Copy the pretrained embeddings if they exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'vocab_size'"
     ]
    }
   ],
   "source": [
    "# Check serialization for _model_ only (should be for experiment as well)\n",
    "import pickle\n",
    "fitted_model = experiment.model\n",
    "\n",
    "model_save_file = 'model.pt'\n",
    "with open(model_save_file, 'wb') as dump_file:\n",
    "    pickle.dump(fitted_model, dump_file)\n",
    "\n",
    "with open(model_save_file, 'rb') as load_file:\n",
    "    loaded_model = pickle.load(load_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: enable your model to process raw data\n",
    "\n",
    "So far, we have been dealing with data wrapped in podium `Dataset` instances. This might not be the case in real-world scenarios, where you want to use a trained model to process raw data.\n",
    "\n",
    "To simplify this, we provide a `Pipeline` class, designed to streamline raw data processing. Pipeline extends your `Experiment` class with the following functionality:\n",
    "1. Obtain predictions from raw data\n",
    "2. Fine-tune your model on raw data\n",
    "3. Retrain your model on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For instance: ['This movie is horrible'], the prediction is: negative, with logits: [-4.63021    4.4030523]\n",
      "For instance: ['This movie is great!'], the prediction is: positive, with logits: [ 4.0780497 -4.1491404]\n"
     ]
    }
   ],
   "source": [
    "from takepod.pipeline import Pipeline\n",
    "\n",
    "ft = experiment.feature_transformer\n",
    "cast_to_torch_transformer = lambda t: torch.from_numpy(ft.transform(t).swapaxes(0,1)).to(device)\n",
    "\n",
    "pipe = Pipeline(\n",
    "  fields = list(fields.values()),\n",
    "  example_format = 'list',\n",
    "  feature_transformer = cast_to_torch_transformer,\n",
    "  model = fitted_model\n",
    "  )\n",
    "\n",
    "instances = [\n",
    "        ['This movie is horrible'], \n",
    "        ['This movie is great!']\n",
    "]\n",
    "\n",
    "for instance in instances:\n",
    "    prediction = pipe.predict_raw(instance)\n",
    "    print(f\"For instance: {instance}, the prediction is: {fields['label'].vocab.itos[prediction.argmax()]}, with logits: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
